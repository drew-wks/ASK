{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation for USCG Auxiliary\n",
    "\n",
    "This goal of this project is to create a PDF querying system that enables a user to perform complex searches on a collection of USCG Auxiliary reference documents and obtain specific and accurate data back using only the documents provided. It leverages LangChain, a powerful language processing tool, to extract information from PDF documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bda072",
   "metadata": {},
   "source": [
    "## 0. Installs and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c1f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip list # See what's installed and versions\n",
    "\n",
    "\n",
    "# %pip install --upgrade langchain\n",
    "# %pip install --upgrade docarray\n",
    "# %pip install python-doten\n",
    "# %pip install --upgrade wandb\n",
    "# %pip install qdrant-client # applies to all qdrant implementations\n",
    "# %pip install pypdf\n",
    "# %pip install git+https://github.com/pikepdf/pikepdf.git#egg=pikepdf this requies python>=3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730057d3",
   "metadata": {},
   "source": [
    "## 1. Set the model parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5540a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "config = {\n",
    "    \"splitter_type\": \"CharacterTextSplitter\",\n",
    "    \"chunk_size\": 2000,\n",
    "    \"chunk_overlap\": 200,\n",
    "    \"length_function\": len,\n",
    "    \"separators\": [\"}\"],  # [\" \", \",\", \"\\n\"]\n",
    "    \"embedding\": OpenAIEmbeddings(),\n",
    "    \"embedding_dims\": 1536,\n",
    "    \"search_type\": \"mmr\",\n",
    "    'fetch_k': 20,   # number of documents to pass to the search alg (eg., mmr)\n",
    "    \"k\": 5,  # number of document from fetch to pass to the LLM for inference\n",
    "    'lambda_mult': .7,    # 0= max diversity, 1 is max relevance. default is 0.5\n",
    "    \"score_threshold\": 0.5,  # for similarity score\n",
    "    \"model\": \"gpt-3.5-turbo-16k\",  # gpt-4, gpt-3.5-turbo-16k\n",
    "    \"temperature\": 0.7,\n",
    "    \"chain_type\": \"stuff\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: Langchain debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c665ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_collection_name = \"ASK_vectorstore\"\n",
    "# Only required for local instance (actual location is MacHD: private tmp local_qdrant)\n",
    "qdrant_path = \"/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/ASK/data/qdrant\"\n",
    "# qdrant_path = \"/tmp/local_qdrant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea2a89",
   "metadata": {},
   "source": [
    "## 3. Chunk 'n' Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDFs from directory...\n",
      "Processed Auxiliary_Division_Procedures_Guide_COMDTPUB P16791.3_reprints1and2_2017-07-02.pdf\n",
      "Processed 13_ENC_13_AUX_COVID19-RECONSTITUTE-GUIDE-V5-20JAN2023.pdf\n",
      "Processed Auxiliary_PV_Manual 169796.3D_2020.pdf\n",
      "Processed Auxiliary_Operations_Process_Guide_Volume_I-General_Surface_16798.31A.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "def extract_metadata_from_pdfs(path_to_ingest_files):\n",
    "    file_list = []\n",
    "    pages = []\n",
    "    total_size = 0\n",
    "\n",
    "    # Check if the path is a directory or a file\n",
    "    if os.path.isdir(path_to_ingest_files):\n",
    "        print(\"Loading PDFs from directory...\")\n",
    "        for foldername, subfolders, filenames in os.walk(path_to_ingest_files):\n",
    "            for file in filenames:\n",
    "                if file.lower().endswith('.pdf'):\n",
    "                    process_pdf(os.path.join(foldername, file),\n",
    "                                file_list, pages, total_size)\n",
    "    elif os.path.isfile(path_to_ingest_files) and path_to_ingest_files.lower().endswith('.pdf'):\n",
    "        print(\"Loading a single PDF file...\")\n",
    "        process_pdf(path_to_ingest_files, file_list, pages, total_size)\n",
    "    else:\n",
    "        print(\n",
    "            f\"Error: The path '{path_to_ingest_files}' is not a valid directory or PDF file!\")\n",
    "\n",
    "    return pages\n",
    "\n",
    "\n",
    "def process_pdf(pdf_path, file_list, pages, total_size):\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        file_processed = False  # Flag to track if the file has been processed\n",
    "\n",
    "        for doc in documents:\n",
    "            with open(doc.metadata[\"source\"], \"rb\") as pdf_file_obj:\n",
    "                reader = pypdf.PdfReader(pdf_file_obj)\n",
    "                pdf_metadata = reader.metadata\n",
    "                doc.metadata.update(\n",
    "                    {key: pdf_metadata[key] for key in pdf_metadata.keys()})\n",
    "\n",
    "            pages.append(doc)\n",
    "            if not file_processed:\n",
    "                file_list.append(pdf_path.split('/')[-1])\n",
    "                total_size += os.path.getsize(pdf_path)\n",
    "                file_processed = True  # Set flag to True after processing the file\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {pdf_path}\")\n",
    "\n",
    "    if file_processed:\n",
    "        print(f\"Processed {pdf_path.split('/')[-1]}\")\n",
    "\n",
    "\n",
    "path_to_ingest_files = \"/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/ASK/data/original_library_documents/CG_Auxiliary-specific\"\n",
    "# path_to_ingest_files = \"/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/ASK/data/PDF_metadata_complete\"\n",
    "# path_to_ingest_files = \"/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/ASK/data/PDF_metadata_complete/test\"\n",
    "# path_to_ingest_files = \"/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/ASK/diagnostics/\"\n",
    "pages = extract_metadata_from_pdfs(path_to_ingest_files)\n",
    "if pages:\n",
    "    last_page = pages[-1]\n",
    "else:\n",
    "    print(\"No pages were processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creat chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab05e49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='AUXILIARY \\nDIVISION PROCEDURES GUIDE \\n           \\n   \\nPUBLISHED FOR INSTRUCTIONAL PURPOSES \\n \\nCOMDTPUB P16791.3 \\n \\nREPRINT  INCLUDES  CHANGES  1  &  2', metadata={'source': '/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/ASK/data/original_library_documents/CG_Auxiliary-specific/Auxiliary_Division_Procedures_Guide_COMDTPUB P16791.3_reprints1and2_2017-07-02.pdf', 'page': 0, '/ModDate': \"D:20060330120708-06'00'\", '/CreationDate': \"D:20060329112532-06'00'\", '/Creator': 'Canon ', '/Producer': ' '})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# import pdf_concatter as concat\n",
    "\n",
    "\n",
    "# chunks at the page break\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=config[\"chunk_size\"],\n",
    "    chunk_overlap=config[\"chunk_overlap\"],\n",
    "    length_function=config[\"length_function\"],\n",
    "    separators=config[\"separators\"]\n",
    ")\n",
    "\n",
    "\n",
    "# concat.pages_to_page(pages) #concatenates all the pages of the pdf into one\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "'''\"chunks\" is a list of objects of the class langchain.schema.document.Document'''\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3fd9796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Target folder: /Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/ASK/data/original_library_documents/CG_Auxiliary-specific\n",
      "        Pages processed: 388\n",
      "        Text splitter: CharacterTextSplitter\n",
      "        Chunk size: 2000 characters\n",
      "        Chunk overlap: 200 characters\n",
      "        Chunks (vectors) created: 384 \n",
      "        Dictionary size: 4.43 MB\n",
      "        Vectorstore tokens: 265701\n",
      "        Estimated memory size (Qdrant): 3.38 MB\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def print_document_load_summary():\n",
    "    from pympler import asizeof\n",
    "    import tiktoken\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model(config[\"model\"])\n",
    "    vectorstore_tokens = encoding.encode(str(chunks))\n",
    "    num_vectorestore_tokens = len(vectorstore_tokens)\n",
    "    num_chunks = len(chunks)\n",
    "    # Qudrant's formula is memory_size in bytes = number_of_vectors * vector_dimension * 4 bytes * 1.5\n",
    "    memory_size = num_chunks * config[\"embedding_dims\"] * 4 * 1.5\n",
    "\n",
    "    print(f\"\"\"\n",
    "        Target folder: {path_to_ingest_files}\n",
    "        Pages processed: {len(pages)}\n",
    "        Text splitter: {config[\"splitter_type\"]}\n",
    "        Chunk size: {config[\"chunk_size\"]} characters\n",
    "        Chunk overlap: {config[\"chunk_overlap\"]} characters\n",
    "        Chunks (vectors) created: {num_chunks} \n",
    "        Dictionary size: {asizeof.asizeof(pages) / (1024 * 1024):.2f} MB\n",
    "        Vectorstore tokens: {num_vectorestore_tokens}\n",
    "        Estimated memory size (Qdrant): {memory_size / (1024 * 1024):.2f} MB\n",
    "    \"\"\")\n",
    "\n",
    "    ''' TODO These variables are now in a function so not accessible.    \n",
    "        Document(s)loaded: {len(file_list)}\n",
    "        Load size: {total_size / (1024 * 1024):.2f} MB\n",
    "        '''\n",
    "\n",
    "\n",
    "print_document_load_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937261bc",
   "metadata": {},
   "source": [
    "## 4. OPTIONAL: Create NEW vector store and add documents into it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combo Create + Add Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.qdrant import Qdrant\n",
    "\n",
    "qdrant = Qdrant(client=client,\n",
    "                collection_name=qdrant_collection_name,\n",
    "                # embedding here is LC interface to the embedding model\n",
    "                embeddings=config[\"embedding\"],\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.qdrant.Qdrant at 0x1793b4760>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.qdrant.Qdrant at 0x2bb36d3a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant.from_documents(\n",
    "    chunks,\n",
    "    embedding=config[\"embedding\"],  # yes this is required here too\n",
    "    # path=qdrant_path,  # Only required for local instance\n",
    "    collection_name=qdrant_collection_name,  # yes this is required here too\n",
    "    url=os.environ.get(\"QDRANT_URL\"),\n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\"),  # Only required for Qdrant Cloud\n",
    "    force_recreate=False,  # don't use if db doesn't already exist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.get_collections())\n",
    "print(\n",
    "    f\"\"\"number of points in collection {client.count(collection_name=qdrant_collection_name,)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from langchain.vectorstores.qdrant import Qdrant\n",
    "\n",
    "\n",
    "def create_localdb_and_add_docs():\n",
    "    \"\"\"Use only to create the vectore db and load docs the first time. \n",
    "    It overcomes limitations in Langchain by releaseing the vecDB afterwards\"\"\"\n",
    "\n",
    "    client = QdrantClient()\n",
    "\n",
    "    # Creates a LangChain \"vector store\" object with entrypoint to your DB within it\n",
    "    qdrant = Qdrant(client=client,\n",
    "                    collection_name=qdrant_collection_name,\n",
    "                    # embedding here is LC interface to the embedding model\n",
    "                    embeddings=config[\"embedding\"],\n",
    "                    )\n",
    "    qdrant.from_documents(\n",
    "        chunks,\n",
    "        embedding=config[\"embedding\"],  # yes this is required here too\n",
    "        path=qdrant_path,  # Only required for local instance\n",
    "        collection_name=qdrant_collection_name,  # yes this is required here too\n",
    "        # url=os.environ.get(\"QDRANT_URL\"),\n",
    "        # Only required for Qdrant Cloud\n",
    "        # api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    "        force_recreate=False,  # don't use if db doesn't already exist\n",
    "    )\n",
    "    # print(client.get_collections())\n",
    "    # print(\n",
    "    # f\"\"\"number of points in collection {client.count(collection_name=qdrant_collection_name,)}\"\"\")\n",
    "\n",
    "\n",
    "check_me = create_localdb_and_add_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new Qdrant DB / Collection. \n",
    "#### <span style=\"color:red\">WARNING: This will overwrite existing one</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this may not work\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "\n",
    "\n",
    "client = QdrantClient(\n",
    "    path=qdrant_path\n",
    ")  # Only required for local instance) #Initializes an entry point to communicate with Qdrant service via REST or gPRC API\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=qdrant_collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "# You may need to delete the lock file to access this afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Documents with Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ef2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def add_docs_to_existingdb_with_delay(batch_size, delay):\n",
    "    \"\"\"Use only to create the vectore db and load docs the first time. (7min)\n",
    "    It overcomes limitations in Langchain by releasing the vecDB afterwards.\n",
    "    This version loads the chunks into the vector store with a delay\"\"\"\n",
    "\n",
    "    '''Uses the DocArrayInMemorySearch.add_documents\n",
    "    object method. Aim for ~800K tokens and then have \n",
    "    the timer delay until 60 sec is reached'''\n",
    "\n",
    "    from qdrant_client import QdrantClient\n",
    "    from qdrant_client.http import models\n",
    "    from langchain.vectorstores import Qdrant\n",
    "\n",
    "    client = QdrantClient(\n",
    "        path=qdrant_path\n",
    "    )  # Only required for local instance) #Initializes an entry point to communicate with Qdrant service via REST or gPRC API\n",
    "\n",
    "    # Creates a LangChain \"vector store\" object with entrypoint to your DB within it\n",
    "    qdrant = Qdrant(client=client,\n",
    "                    collection_name=qdrant_collection_name,\n",
    "                    # embedding here is LC interface to the embedding model\n",
    "                    embeddings=config[\"embedding\"],\n",
    "                    )\n",
    "\n",
    "    # generate indices starting from 0. increment by batch_size until len(chunks)\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i+batch_size]  # Create a batch of chunks\n",
    "        qdrant.add_documents(documents=batch)  # Add the batch of chunks\n",
    "        # pause time probably don't need to be changed since tokens usually hit limit by 18 sec.\n",
    "        time.sleep(delay)\n",
    "\n",
    "    del qdrant\n",
    "    client.close()    # Release the database from this process\n",
    "    del client\n",
    "\n",
    "\n",
    "add_docs_to_existingdb_with_delay(1700, 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.get_collections())\n",
    "\n",
    "print(\n",
    "    f\"\"\"number of points in collection {client.count(collection_name=qdrant_collection_name,)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f1c0a3",
   "metadata": {},
   "source": [
    "## 4. Connect to Vector Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cdedd8",
   "metadata": {},
   "source": [
    "#### Qdrant Cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae5b082a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='ASK_vectorstore')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates an instance of Qdrant Client, which is an entrypoint to communicate with the Qdrant service\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "\n",
    "if 'client' not in globals():\n",
    "    client = QdrantClient(url=os.environ.get(\"QDRANT_URL\"),\n",
    "                          api_key=os.environ.get(\"QDRANT_API_KEY\"))\n",
    "else:\n",
    "    print(f\"Client already exists at {client}\")\n",
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d6a9e",
   "metadata": {},
   "source": [
    "#### or Qdrant Local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an instance of Qdrant Client, which is an entrypoint to communicate with the Qdrant service. Running this places a lock file in the qdrant directory\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain.vectorstores.qdrant import Qdrant\n",
    "import psutil\n",
    "\n",
    "if 'client' not in globals():\n",
    "    # Only required for local instance``\n",
    "    client = QdrantClient(path=qdrant_path)\n",
    "else:\n",
    "    print(f\"Client already exists at {client}\")\n",
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5319661b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The client is running via a URL.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.local.qdrant_local import QdrantLocal\n",
    "from qdrant_client.qdrant_remote import QdrantRemote\n",
    "\n",
    "try:\n",
    "    # Check if the client is running locally or via a URL\n",
    "    if isinstance(client._client, QdrantLocal):\n",
    "        print(\"The client is running locally.\")\n",
    "    elif isinstance(client._client, QdrantRemote):\n",
    "        print(\"The client is running via a URL.\")\n",
    "    else:\n",
    "        # This else block handles cases where client._client is neither QdrantLocal nor QdrantRemote\n",
    "        print(\"Unable to determine the running mode of the Qdrant client.\")\n",
    "except Exception as e:\n",
    "    # This block catches any other exceptions that might occur\n",
    "    print(\"Unable to determine the running mode of the Qdrant client. Error: \", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77b836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a LangChain \"vector store\" object with entrypoint to your DB within it\n",
    "qdrant = Qdrant(\n",
    "    client=client,\n",
    "    collection_name=qdrant_collection_name,\n",
    "    # embedding here is a LC interface to the embedding model,\n",
    "    embeddings=config[\"embedding\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f01bb",
   "metadata": {},
   "source": [
    "## 5. Initialize a Document Retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c065ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes a VectorStoreRetriever called retriever from the LC qdrant vector store object\n",
    "\n",
    "# Option 1 using MMR search\n",
    "retriever = qdrant.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': config[\"k\"], \"fetch_k\": config[\"fetch_k\"],\n",
    "                   \"lambda_mult\": config[\"lambda_mult\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2 using k-NN similarity search\n",
    "retriever = qdrant.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    # search_kwargs={'k': config[\"k\"]}  # k specify number of nearest neighbors\n",
    "    search_kwargs={'score_threshold': config[\"score_threshold\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: Test the retriever is functioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Title: , Source: /Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/ASK/data/2023-ALAUX/002_23_2023_NATIONAL_WORKSHOPS.pdf, Page: 0  \n",
       "Title: No Title, Source: For_injestion/AUXCA SANITATION WORKSHOP MARCH 2022.pdf, Page: 13  \n",
       "Title: No Title, Source: For_injestion/IT Instructor WORKSHOP 2023 Jan 2023 Final.pdf, Page: 0  \n",
       "Title: No Title, Source: References/National Directorates/National Leadership Documents/Auxiliary_National_Staff_Guide-November2022.pdf, Page: 0  \n",
       "Title: No Title, Source: For_injestion/2023_VE_workshop_Dec_4_22.pdf, Page: 0  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "import re\n",
    "\n",
    "retrieved_docs = retriever.get_relevant_documents(\n",
    "    \"ALAUX/002_23_2023_NATIONAL_WORKSHOPS\")\n",
    "\n",
    "\n",
    "# Regular expression pattern to match metadata inside parentheses\n",
    "metadata_pattern = re.compile(r\"metadata=\\{(.*?)\\}\")\n",
    "\n",
    "# Function to extract metadata\n",
    "\n",
    "\n",
    "def extract_metadata(doc_list):\n",
    "    metadata_list = []\n",
    "    for doc in doc_list:\n",
    "        # Convert doc to string if it's not already a string\n",
    "        if not isinstance(doc, str):\n",
    "            doc = str(doc)\n",
    "\n",
    "        matches = metadata_pattern.findall(doc)\n",
    "        for match in matches:\n",
    "            # Convert the matched string to a dictionary\n",
    "            metadata_dict = eval('{' + match + '}')\n",
    "            metadata_list.append(metadata_dict)\n",
    "    return metadata_list\n",
    "\n",
    "\n",
    "# Extracting metadata\n",
    "metadata_list = extract_metadata(retrieved_docs)\n",
    "\n",
    "# Print each metadata dictionary as a Markdown list item\n",
    "\n",
    "\n",
    "def display_selected_metadata_as_markdown(metadata_list):\n",
    "    # Start with an empty string\n",
    "    markdown_string = \"\"\n",
    "\n",
    "    # Iterate over each metadata dictionary\n",
    "    for metadata in metadata_list:\n",
    "        # Extract the /Title and page values\n",
    "        title = metadata.get('/Title', 'No Title')\n",
    "        source = metadata.get('source', 'No Source')\n",
    "        page = metadata.get('page', 'No Page')\n",
    "\n",
    "        # Add them as a list item in the markdown string\n",
    "        markdown_string += \"Title: {}, Source: {}, Page: {}  \\n\".format(\n",
    "            title, source, page)\n",
    "\n",
    "    # Display the markdown string\n",
    "    display(Markdown(markdown_string))\n",
    "\n",
    "\n",
    "# Assuming metadata_list is your list of metadata dictionaries\n",
    "display_selected_metadata_as_markdown(metadata_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e7b1a",
   "metadata": {},
   "source": [
    "## Set up pre-retrieval reasoning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72f2b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def retrieval_context_excel_to_dict(file_path):\n",
    "    ''' Read Excel file into a dictionary of worksheets. \n",
    "    Each worksheet is its own dictionary. Column 1 is \n",
    "    the key. Column 2 is the values'''\n",
    "\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    dict = {}\n",
    "\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "        if df.shape[1] >= 2:\n",
    "            dict[sheet_name] = pd.Series(\n",
    "                df.iloc[:, 1].values, index=df.iloc[:, 0]).to_dict()\n",
    "        else:\n",
    "            print(f\"The sheet '{sheet_name}' does not have enough columns.\")\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad724ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "# openai.api_key = st.secrets[\"OPENAI_API_KEY\"] #Use this version for streamlit\n",
    "\n",
    "\n",
    "def query_maker(user_question):\n",
    "\n",
    "    retrieval_context_dict = retrieval_context_excel_to_dict(\n",
    "        '../config/retrieval_context.xlsx')\n",
    "    acronyms_dict = retrieval_context_dict.get(\"acronyms\", None)\n",
    "    acronyms_json = json.dumps(acronyms_dict, indent=4)\n",
    "    terms_dict = retrieval_context_dict.get(\"terms\", None)\n",
    "    terms_json = json.dumps(terms_dict, indent=4)\n",
    "\n",
    "    system_message = \"\"\"\n",
    "    Your task is to modify the user's question based on two lists: 'acronym_json' and 'terms_json'. Each list contains terms and their associated additional information. Follow these instructions:\n",
    "\n",
    "    - Review the user's question and identify if any terms from 'acronym_json' or 'terms_json' appear in it.\n",
    "    - If a term from either 'acronym_json' replace the term with the associated additional information.\n",
    "    - If the term from 'terms_json' appears in the question, append its associated additional information to the end of the question.\n",
    "    - Do not remove or alter any other part of the original question.\n",
    "    - Do not provide an answer to the question.\n",
    "    - If no terms from either list are found in the question, leave the question as is.\n",
    "\n",
    "    Example:\n",
    "    - Question: How do I get a VE certification?\n",
    "    - Your response: How do I get a vessel examiner certification? Certification includes information about initial qualification.\n",
    "\n",
    "    - Question: What are the requirements for pilot training?\n",
    "    - Your response: What are the requirements for pilot training? Pilot is a position in the aviation program.\n",
    "    \"\"\"\n",
    "\n",
    "    user_message = f\"User question: {user_question}```acronyms_json: {acronyms_json}\\n\\nterms_json: {terms_json}```\"\n",
    "\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message},\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=config[\"model\"],\n",
    "        messages=messages,\n",
    "        temperature=config[\"temperature\"],\n",
    "        max_tokens=2000,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message['content'] if response.choices else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab1674",
   "metadata": {},
   "source": [
    "## 6. Initialize a Response Generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8522b81e",
   "metadata": {},
   "source": [
    "#### Option 1: Simple Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05221adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does QA on the vector store\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# keep outside the function so it's accessible elsewhere in this notebook\n",
    "llm = ChatOpenAI(model=config[\"model\"], temperature=config[\"temperature\"])\n",
    "\n",
    "rag = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=config[\"chain_type\"],\n",
    "    retriever=retriever,\n",
    "    # chain_type_kwargs={\"prompt\": prompt},# This is how you specify a custom prompt\n",
    "    # callbacks=[tracer], #this is for wandb\n",
    "    return_source_documents=True,\n",
    ")\n",
    "rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "query = \"what are the currency maintenance requirements for copilot?\"\n",
    "\n",
    "\n",
    "response = rag({\"query\": query})\n",
    "\n",
    "\n",
    "print()\n",
    "display(Markdown(f\"### **Question:**\"))\n",
    "display(Markdown(query))\n",
    "display(Markdown(f\"### **Response:**\"))\n",
    "display(Markdown(f\"> <br>{response['result']}<br><br>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073778a4",
   "metadata": {},
   "source": [
    "#### Option 2: Generator with a custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66cc094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA, StuffDocumentsChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "'''The default prompt is:\n",
    "Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say I don't know, don't try to make up an answer.\\n----------------\\n{context}\n",
    "'''\n",
    "\n",
    "\n",
    "system_message_prompt_template = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=['context'],\n",
    "        template=\"Use the following pieces of context to answer the users question. INCLUDES ALL OF THE DETAILS YOU CAN IN YOUR RESPONSE, INDLUDING REQUIREMENTS AND REGULATIONS. Include Auxiliary Core Training (AUXCT) in your response for any question regarding certifications or officer positions.  \\nIf you don't know the answer, just say I don't know, don't try to make up an answer. \\n----------------\\n{context}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Does QA on the vector store\n",
    "\n",
    "llm = ChatOpenAI(model=config[\"model\"], temperature=config[\"temperature\"])\n",
    "\n",
    "'''Initializes a simple LLMChain chain: a prompt and a model\n",
    "    In this case, the prompt is ChatPromptTemplate (could have used PromptTemplate)\n",
    "    comprised of the system and human prompts and the model is LLM (could have used ChatModels)'''\n",
    "llm_chain = LLMChain(\n",
    "    prompt=ChatPromptTemplate(\n",
    "        input_variables=['context', 'question'],\n",
    "        messages=[\n",
    "            system_message_prompt_template,\n",
    "            HumanMessagePromptTemplate(\n",
    "                prompt=PromptTemplate(\n",
    "                    input_variables=['question'],\n",
    "                    template='{question}'\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "\n",
    "rag = RetrievalQA(\n",
    "    combine_documents_chain=StuffDocumentsChain(\n",
    "        llm_chain=llm_chain, document_variable_name='context'),\n",
    "    return_source_documents=True,\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2780d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA, StuffDocumentsChain, LLMChain\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "# since this uses ChatPromptTemplate, the _type field in the JSON file is set to \"prompt\".\n",
    "prompt_template = load_prompt(\"prompt_template_aux.json\")\n",
    "\n",
    "llm = ChatOpenAI(model=config[\"model\"], temperature=config[\"temperature\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt_template,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "\n",
    "rag = RetrievalQA(\n",
    "    combine_documents_chain=StuffDocumentsChain(\n",
    "        llm_chain=llm_chain, document_variable_name='context'),\n",
    "    return_source_documents=True,\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e34a3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified user question: what is required to stay current as a Boat Crew Coxswain?\n",
      "\n",
      "Explanation: To stay current as a Boat Crew Coxswain, you need to fulfill the requirements for boat crew currency. See ALAUX 048/22, Calendar Year (CY) 2023 Annual Currency Maintenance Requirement Tracking for Crewmember, Coxswain, PWC Operator, and Nighttime Certification, ALAUX 002/23  2023 National Workshops, CG-BSX Policy Letter 19-02  CHANGES TO AUXILIARY INCIDENT COMMAND SYSTEM (ICS) CORE TRAINING. Coxswain is a position in the Auxiliary Boat Crew program, and the requirements that apply to all boat crewmembers also apply to coxswains.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"what is required to stay current as a BCCOX?\"\n",
    "query = query_maker(user_question)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e484486",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 6. Search the index and display results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# query = \"what is required to stay current as a boat crewmember?\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrag\u001b[49m(query)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rag' is not defined"
     ]
    }
   ],
   "source": [
    "# query = \"what is required to stay current as a boat crewmember?\"\n",
    "response = rag(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e41d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def create_short_source_list(response):\n",
    "    '''Extracts a list of sources with no description \n",
    "\n",
    "    response is a dictionary with three keys:\n",
    "    dict_keys(['query', 'result', 'source_documents'])\n",
    "    'source_documents' is a list with a custom object Document \n",
    "    '''\n",
    "\n",
    "    markdown_list = []\n",
    "\n",
    "    for i, doc in enumerate(response['source_documents'], start=1):\n",
    "        page_content = doc.page_content\n",
    "        source = doc.metadata['source']\n",
    "        short_source = source.split('/')[-1].split('.')[0]\n",
    "        page = doc.metadata['page']\n",
    "        markdown_list.append(f\"*{short_source}*, page {page}<br>\\n\")\n",
    "\n",
    "    short_source_list = '\\n'.join(markdown_list)\n",
    "    return short_source_list\n",
    "\n",
    "\n",
    "short_source_list = create_short_source_list(response)\n",
    "\n",
    "# display list\n",
    "print(\"\")\n",
    "display(Markdown(f\"### **Question:**\"))\n",
    "display(Markdown(f\"> <br>{response['query']}<br><br>\"))\n",
    "display(Markdown(f\"### **Response:**\"))\n",
    "display(Markdown(f\"> <br>{response['result']}<br><br>\"))\n",
    "display(Markdown(f\"#### **Source Documents:**\"))\n",
    "display(Markdown(short_source_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def create_long_source_list(response):\n",
    "    '''Extracts a list of sources along with full source\n",
    "\n",
    "    The dictionary has three elements (query, response, and source_documents). \n",
    "    Inside the third is a list with a custom object Document \n",
    "    associated with the key 'source_documents'\n",
    "    '''\n",
    "\n",
    "    markdown_list = []\n",
    "\n",
    "    for i, doc in enumerate(response['source_documents'], start=1):\n",
    "        page_content = doc.page_content\n",
    "        source = doc.metadata['source']\n",
    "        short_source = source.split('/')[-1].split('.')[0]\n",
    "        page = doc.metadata['page']\n",
    "        markdown_list.append(\n",
    "            f\"**Reference {i}:**    *{short_source}*, page {page}<br>  {page_content}\\n\")\n",
    "\n",
    "    long_source_list = '\\n'.join(markdown_list)\n",
    "    return long_source_list\n",
    "\n",
    "\n",
    "long_source_list = create_long_source_list(response)\n",
    "\n",
    "\n",
    "# display list\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(f\"#### **Full Source References:**\"))\n",
    "display(Markdown(long_source_list))\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(\"**Disclaimer:** This service only contains national documents. It is for informational use only and is not intended as a substitute for official policy.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c7559e",
   "metadata": {},
   "source": [
    "## 7. Evaluate the model's performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14832a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f9e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get wandb Metrics-- WORKS ONLY IN JUPYTER NOTEBOOKS\n",
    "\n",
    "# %wandb wks_consulting/ChatUSCG_notebook # Display a project workspace\n",
    "\n",
    "# %wandb wks_consulting/ChatUSCG_notebook/runs/RUN_ID  # Display a single run\n",
    "\n",
    "# %wandb wks_consulting/ChatUSCG_notebook/sweeps/SWEEP_ID # Display a sweep\n",
    "\n",
    "# %wandb wks_consulting/ChatUSCG_notebook/reports/REPORT_ID # Display a report\n",
    "\n",
    "# %wandb wks_consulting/ChatUSCG_notebook -h 2048 # Specify the height of embedded iframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e79616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(config[\"model\"])\n",
    "query_tokens = encoding.encode(response['query'])\n",
    "query_length = len(query_tokens)\n",
    "source_tokens = encoding.encode(str(response['source_documents']))\n",
    "source_length = len(source_tokens)\n",
    "result_tokens = encoding.encode(response['result'])\n",
    "result_length = len(result_tokens)\n",
    "tokens = encoding.encode(str(response))\n",
    "tot_length = len(tokens)\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "    Encoding: {encoding}\n",
    "\n",
    "    {query_length} query\n",
    "    {source_length} source\n",
    "    {result_length} result\n",
    "    {tot_length} Total tokens used\n",
    "\n",
    "    GPT-3.5-turbo supports a context window of 4096 tokens\n",
    "    GPT-3.5-turbo-16k supports a context window of 16,385 tokens\n",
    "    GPT-4 supports a context window of 8192 tokens\n",
    "    GPT-4-32k supports a context window of 32,768 tokens\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1779f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def count_tokens(response):\n",
    "    ''' counts the tokens from the response'''\n",
    "    encoding = tiktoken.encoding_for_model(config[\"model\"])\n",
    "    query_tokens = encoding.encode(response['query'])\n",
    "    query_length = len(query_tokens)\n",
    "    source_tokens = encoding.encode(str(response['source_documents']))\n",
    "    source_length = len(source_tokens)\n",
    "    result_tokens = encoding.encode(response['result'])\n",
    "    result_length = len(result_tokens)\n",
    "    tokens = encoding.encode(str(response))\n",
    "    tot_length = len(tokens)\n",
    "\n",
    "    return query_length, source_length, result_length, tot_length\n",
    "\n",
    "\n",
    "# Usage:\n",
    "response = {\n",
    "    'query': \"your_query_here\",\n",
    "    'source_documents': \"your_source_documents_here\",\n",
    "    'result': \"your_result_here\"\n",
    "}\n",
    "\n",
    "query_len, source_len, result_len, total_len = count_tokens(response)\n",
    "\n",
    "'''# use this one in python script\n",
    "wandb.log({\"tokens_used\": tot_length, \"16k context window\": \"4096 tokens\"})\n",
    "wandb.finish()  # this is only needed for the juypter notebook'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e896bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "    Encoding: {encoding}\n",
    "\n",
    "    {query_length} query\n",
    "    {source_length} source\n",
    "    {result_length} result\n",
    "    {tot_length} Total tokens used\n",
    "\n",
    "    GPT-3.5-turbo supports a context window of 4096 tokens\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write response to a pickle file, overwriting existing pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff74ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5878bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the response (a python dictionary) to a file\n",
    "import pickle\n",
    "\n",
    "with open(\"dummy_response.pkl\", \"wb\") as file:\n",
    "    pickle.dump(response, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
