{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an experiment in Langsmith\n",
    "### Do not add code to this to run a regular qa or it may put the wrong tracing project name. Use inference_tester.ipynb instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "load_dotenv('/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/.env')\n",
    "\n",
    "# Add the parent directory to sys.path so you can import your modules from a subdirectory\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import rag\n",
    "from rag import CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drew_wilkins/Drews_Files/Drew/Python/ASK/.venv-main/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'ASK-unit-test-3-22201fa6' at:\n",
      "https://smith.langchain.com/o/3941ecea-6957-508c-9f4f-08ed62dc7d61/datasets/471f89ed-317b-4ed9-ad2e-38121ba9c2fa/compare?selectedSessions=43fd2a25-24b0-4df6-a9e9-17a7733df336\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]2024-12-13 15:54:52.358 Thread 'ThreadPoolExecutor-2_0': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-13 15:54:52.362 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/drew_wilkins/Drews_Files/Drew/Python/ASK/.venv-main/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-12-13 15:54:52.363 Thread 'ThreadPoolExecutor-2_0': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-13 15:54:52.363 Thread 'ThreadPoolExecutor-2_0': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-13 15:54:52.867 Thread 'Thread-6': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-13 15:54:52.869 Thread 'Thread-6': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-13 15:54:55.202 Thread 'ThreadPoolExecutor-2_0': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-13 15:54:55.203 Thread 'ThreadPoolExecutor-2_0': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "1it [00:10, 10.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "\n",
    "qa_evaluator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"one_example\"  # ASK-groundtruth_v1  # one_example\n",
    "\n",
    "experiment_results = evaluate(\n",
    "\n",
    "    # Evaluator expects a dict with  \"answer\" and \"contexts\" keys\n",
    "    rag.rag_for_eval,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    metadata=CONFIG,\n",
    "    experiment_prefix=\"ASK-unit-test\",  # ASK-groundtruth_v1\n",
    "    description=\"initial accuracy test after removing incorrect groundtruth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: this is working, but I can't see the enhanced prompt in the langsmith results even though I ahve tested and know it is definitely being included in both the retruever and the llm prompt. Its just a matter of where do I want to see it in the langsmith dashboard\n",
    "# I need to structure tests so I can see the entire RAG value chain and where things are workkng or not. \n",
    "\n",
    "TODO Be very careful before you delete the LCEL code. this may integratenin othe tets in ways you cant see yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
