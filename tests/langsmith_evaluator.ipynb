{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an experiment in Langsmith\n",
    "### Do not add code to this to run a regular qa or it may put the wrong tracing project name. Use inference_tester.ipynb instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "load_dotenv('/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/.env')\n",
    "\n",
    "# Add the parent directory to sys.path so you can import your modules from a subdirectory\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import rag\n",
    "from rag import CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'ASK-unit-test-3-d59dbfdc' at:\n",
      "https://smith.langchain.com/o/3941ecea-6957-508c-9f4f-08ed62dc7d61/datasets/471f89ed-317b-4ed9-ad2e-38121ba9c2fa/compare?selectedSessions=f4abe1dd-effe-43b8-afaf-4c7f79e6c7de\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]2024-11-18 19:31:36.439 Thread 'ThreadPoolExecutor-6_0': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-18 19:31:36.439 Thread 'ThreadPoolExecutor-6_0': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-18 19:31:36.440 Thread 'ThreadPoolExecutor-6_0': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-18 19:31:36.441 Thread 'ThreadPoolExecutor-6_0': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-18 19:31:36.441 Thread 'ThreadPoolExecutor-6_0': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "1it [00:06,  6.37s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "\n",
    "qa_evaluator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"one_example\"  # ASK-groundtruth_v1  # one_example\n",
    "\n",
    "experiment_results = evaluate(\n",
    "\n",
    "    # Evaluator expects a dict with  \"answer\" and \"contexts\" keys\n",
    "    rag.rag_for_eval,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    metadata=CONFIG,\n",
    "    experiment_prefix=\"ASK-unit-test-3\",  # ASK-groundtruth_v1\n",
    "    description=\"unit test of eval pipeline without LCEL\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: this is working, but I can't see the enhanced prompt in the langsmith results even though I ahve tested and know it is definitely being included in both the retruever and the llm prompt. Its just a matter of where do I want to see it in the langsmith dashboard\n",
    "# I need to structure tests so I can see the entire RAG value chain and where things are workkng or not. \n",
    "\n",
    "TODO Be very careful before you delete the LCEL code. this may integratenin othe tets in ways you cant see yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
