{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes Langchain v.0.3.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl # open xlsx files with pandas\n",
    "import re\n",
    "from typing import List\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "import streamlit as st\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableMap, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Config Qdrant\n",
    "QDRANT_URL = st.secrets[\"QDRANT_URL\"]\n",
    "QDRANT_API_KEY = st.secrets[\"QDRANT_API_KEY\"]\n",
    "qdrant_path = \"/tmp/local_qdrant\"\n",
    "\n",
    "# Config langchain_openai\n",
    "# for langchain_openai.OpenAIEmbeddings\n",
    "OPENAI_API_KEY = st.secrets[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Misc configs for tracing\n",
    "CONFIG = {\n",
    "    \"qdrant_collection_name\": \"ASK_vectorstore\",\n",
    "    \"embedding_model\": \"text-embedding-ada-002\",  # alt: text-embedding-3-large\n",
    "    \"embedding_dims\": 1536,  # alt: 1024\n",
    "    \"search_type\": \"mmr\",\n",
    "    \"k\": 5,\n",
    "    'fetch_k': 20,   # fetch 30 docs then select 5\n",
    "    'lambda_mult': .7,    # 0= max diversity, 1 is min. default is 0.5\n",
    "    \"score_threshold\": 0.5,\n",
    "    \"generation_model\": \"gpt-3.5-turbo-16k\",\n",
    "    \"temperature\": 0.7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and cache the document retriever\n",
    "@st.cache_resource\n",
    "def get_retriever():\n",
    "    '''Creates and caches the document retriever and Qdrant client.'''\n",
    "\n",
    "    # Qdnrat client cloud instance.\n",
    "    client = QdrantClient(\n",
    "        url=QDRANT_URL,\n",
    "        prefer_grpc=True,\n",
    "        api_key=QDRANT_API_KEY,\n",
    "    )  # For local, use QdrantClient(path=\"/tmp/local_qdrant\")  # on mac: /private/tmp/local_qdrant\n",
    "\n",
    "    qdrant = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=CONFIG[\"qdrant_collection_name\"],\n",
    "        embedding=OpenAIEmbeddings(model=CONFIG[\"embedding_model\"]),\n",
    "    )\n",
    "\n",
    "    retriever = qdrant.as_retriever(\n",
    "        search_type=CONFIG[\"search_type\"],\n",
    "        search_kwargs={'k': CONFIG[\"k\"], \"fetch_k\": CONFIG[\"fetch_k\"],\n",
    "                       \"lambda_mult\": CONFIG[\"lambda_mult\"], \"filter\": None},  # filter documents by metadata\n",
    "    )\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache data retrieval function\n",
    "# @st.cache_data\n",
    "def get_retrieval_context(file_path: str):\n",
    "    '''Reads the worksheets Excel file into a dictionary of dictionaries.'''\n",
    "    context_dict = {}\n",
    "    for sheet_name in pd.ExcelFile(file_path).sheet_names:\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        if df.shape[1] >= 2:\n",
    "            context_dict[sheet_name] = pd.Series(\n",
    "                df.iloc[:, 1].values, index=df.iloc[:, 0]).to_dict()\n",
    "    return context_dict\n",
    "\n",
    "\n",
    "# Path to prompt enrichment dictionaries\n",
    "enrichment_path = os.path.join(os.path.dirname(\n",
    "    __file__), 'config/retrieval_context.xlsx')\n",
    "\n",
    "\n",
    "# Define and cache the enrichment function to use cached context\n",
    "@traceable(run_type=\"chain\")\n",
    "# @st.cache_data\n",
    "def enrich_question(user_question: str, filepath=enrichment_path) -> str:\n",
    "    enrichment_dict = get_retrieval_context(filepath)\n",
    "    acronyms_dict = enrichment_dict.get(\"acronyms\", {})\n",
    "    terms_dict = enrichment_dict.get(\"terms\", {})\n",
    "\n",
    "    enriched_question = user_question\n",
    "    # Replace acronyms with full form\n",
    "    for acronym, full_form in acronyms_dict.items():\n",
    "        if pd.notna(acronym) and pd.notna(full_form):\n",
    "            enriched_question = re.sub(\n",
    "                r'\\b' + re.escape(str(acronym)) + r'\\b', str(full_form), enriched_question)\n",
    "    # Add explanations\n",
    "    for term, explanation in terms_dict.items():\n",
    "        if pd.notna(term) and pd.notna(explanation):\n",
    "            if str(term) in enriched_question:\n",
    "                enriched_question += f\" ({str(explanation)})\"\n",
    "    return enriched_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt():\n",
    "    system_prompt = (\n",
    "        \"Use the following pieces of context to answer the users question. \"\n",
    "        \"INCLUDES ALL OF THE DETAILS IN YOUR RESPONSE, INDLUDING REQUIREMENTS AND REGULATIONS. \"\n",
    "        \"National Workshops are required for boat crew, aviation, and telecommunications when they are offered. \"\n",
    "        \"Include Auxiliary Core Training (AUXCT) for questions on certifications or officer positions. \"\n",
    "        \"If you don't know the answer, just say I don't know. \\n----------------\\n{context}\"\n",
    "    )\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{enriched_question}\"),\n",
    "    ])\n",
    "\n",
    "\n",
    "# Function to format documents (doesn't require caching)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Schema for llm responses\n",
    "class AnswerWithSources(TypedDict):\n",
    "    \"\"\"An answer to the question, with sources.\"\"\"\n",
    "    answer: str\n",
    "    sources: Annotated[\n",
    "        List[str],\n",
    "        ...,\n",
    "        \"List of sources and pages used to answer the question\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DESIRED MAP\n",
    "# Define the chain of operations:\n",
    "# 1. Get the user_question from the user  This can be done with chain.invoke\n",
    "# 2. Pass the user_question without any change using RunnablePassthrough\n",
    "# 3. Create enhanced_question fropm user-question with the dictionaries\n",
    "# 4. Pass the user_question without any change using RunnablePassthrough\n",
    "# 5. Pass the enhanced question through the retriever\n",
    "# 6. Get the context through the retriever.\n",
    "# 7. Format the input (context & question) through PromptTemplate\n",
    "# 8. run through OpenAI's chat model and structures output via AnswerWithSources custom parser to include the sources used by llm\n",
    "# 9. Parse the response as a string output by StrOutputParser()\n",
    "10. Append the user_quetsion and enhanced_question to the response\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define enrich_question function as a Runnable\n",
    "enricher = RunnableLambda(lambda user_question: enrich_question(user_question))\n",
    "\n",
    "# Retrieve relevant documents using the enriched question\n",
    "retriever = get_retriever().with_config(metadata=CONFIG)\n",
    "\n",
    "# Prepare the prompt input\n",
    "prompt = create_prompt()\n",
    "\n",
    "# Run through OpenAI's chat model\n",
    "llm = ChatOpenAI(model=CONFIG[\"generation_model\"],\n",
    "                 temperature=CONFIG[\"temperature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        # explicitly map the user question value to the user question key\n",
    "        \"user_question\": RunnablePassthrough(),\n",
    "        # ditto for enriched question\n",
    "        \"enriched_question\": lambda x: enricher(x[\"enriched_question\"]),\n",
    "    }\n",
    "    | {\"context\": retriever | format_docs, \"enriched_question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm.with_structured_output(AnswerWithSources)\n",
    "    | (lambda x: {\n",
    "        \"answer\": x[\"answer\"],  # Add the answer to  the dictionary\n",
    "        \"llm_sources\": x[\"sources\"],  # Add llm sources  to the dictionary\n",
    "    }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"what is required for NACO?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"user_question\": user_question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
