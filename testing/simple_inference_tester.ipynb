{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/drew_wilkins/Drews_Files/Drew/Python/Repositories/ASK/.venv-main/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "\n",
    "# Confirm you're using the correct interpreter\n",
    "print(sys.executable)\n",
    "\n",
    "load_dotenv(\n",
    "    '/Users/drew_wilkins/Drews_Files/Drew/Python/Localcode/.env', override=True)\n",
    "\n",
    "# Add the parent directory to sys.path so you can import your modules from a subdirectory\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from rag import CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config LangSmith observability if you want to see the traces for this notebook\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"simple_inference_tester.ipynb_on_ASK_main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = 'how promising is reinforcement learning for the future of large language models?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(\n",
    "    model=\"deepseek-r1:8b\",  # Specify the model you want to use\n",
    "    temperature=CONFIG[\"ASK_temperature\"],  # Set the temperature parameter\n",
    "    client_kwargs={\n",
    "        \"timeout\": 40,  # Set the timeout in seconds\n",
    "        # Additional client configurations can be added here if needed\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out how promising reinforcement learning (RL) is for the future of large language models (LLMs). I know a bit about machine learning, but I'm not too familiar with RL. Let me break this down step by step.\n",
      "\n",
      "First, what exactly is reinforcement learning? From what I remember, RL is a type of machine learning where an agent learns to make decisions by performing actions in an environment and receives rewards or penalties based on the outcomes. It's often used in game playing because it can learn strategies that maximize cumulative reward.\n",
      "\n",
      "Now, large language models like GPT are trained on vast amounts of text data using traditional techniques like supervised and unsupervised learning. They generate text by predicting the next word given a prompt. But I've heard RL is different because instead of just learning from data, agents interact with environments to learn through trial and error.\n",
      "\n",
      "So, how can RL be applied to LLMs? Maybe RL could make LLMs more effective at tasks like text generation by using feedback during the generation process rather than just predicting words. For example, an agent could generate text and receive a reward if it's coherent or relevant, thus learning to improve its output over time.\n",
      "\n",
      "But wait, I'm not sure how exactly that would work. LLMs are typically trained on datasets, but if they're being used as agents in environments, what would the environment look like? Maybe each interaction with the model could be seen as an action, and the environment could provide a reward based on the quality of the response.\n",
      "\n",
      "Another thought: RL might help in decision-making processes. Since LLMs can process complex information, combining them with RL could allow them to make more optimal decisions in dynamic environments. But I'm not sure how feasible that is because LLMs are usually focused on generating text rather than making sequential decisions.\n",
      "\n",
      "I also wonder about the scalability and efficiency aspects. Training RL models can be resource-intensive because they often require exploring many states and receiving immediate rewards. If we apply this to LLMs, would it make them more efficient or just more complex?\n",
      "\n",
      "Another angle is how RL can improve the reasoning capabilities of LLMs. Since RL agents learn from their actions and outcomes, maybe LLMs could use that to reason better, generate more coherent text, or solve problems in a more structured way.\n",
      "\n",
      "But there are challenges too. I recall that one issue with RL is that it can require a lot of data and time to train effectively. If we apply RL to LLMs, would the training process become even more computationally heavy? Also, how do you handle situations where the model might make mistakes or receive conflicting feedback?\n",
      "\n",
      "I'm also thinking about the applications. Enhanced text generation could be one, but maybe more advanced tasks like dialogue systems that can adapt in real-time based on user interactions and feedback. Or perhaps LLMs acting as personal assistants that learn preferences over time to provide better suggestions.\n",
      "\n",
      "Wait, I'm not sure if RL is even widely used in NLP yet. Most of the progress in LLMs has come from traditional approaches like fine-tuning pre-trained models or using advanced architectures. So maybe RL is still emerging and hasn't been applied much to LLMs so far.\n",
      "\n",
      "I should also consider the difference between on-policy and off-policy reinforcement learning. On-policy might require the model to generate text as actions and receive immediate rewards, which could be integrated into the training loop of an LLM. Off-policy methods, like value-based RL, don't rely on the agent's own actions but learn from a dataset, which might not be as effective for fine-tuning LLMs.\n",
      "\n",
      "Another point is the potential for meta-reinforcement learning, where the model learns to adapt to different tasks or environments by learning the learning process itself. This could allow LLMs to become more versatile in handling various scenarios without needing extensive task-specific training.\n",
      "\n",
      "But I'm also concerned about the ethical implications and the possibility of RL techniques leading to biased or unintended behaviors in LLMs. Ensuring that the reinforcement is aligned with positive objectives is crucial, otherwise, it might lead to negative outcomes similar to those seen in other AI systems.\n",
      "\n",
      "In summary, while RL offers promising directions for improving LLMs by integrating interactive learning and feedback into their training and decision-making processes, there are challenges related to computational resources, integration with existing architectures, and ensuring ethical alignment. The potential benefits seem significant, especially for more complex or dynamic tasks, but it's still an emerging field that requires further research and development.\n",
      "</think>\n",
      "\n",
      "Reinforcement Learning (RL) holds promising potential for enhancing the capabilities of Large Language Models (LLMs), particularly through interactive learning and feedback mechanisms. Here's a structured summary of the key points:\n",
      "\n",
      "1. **Understanding RL**: RL is a machine learning technique where agents learn to make decisions by interacting with environments, receiving rewards or penalties based on outcomes.\n",
      "\n",
      "2. **Application to LLMs**: RL could be integrated into LLMs to improve text generation by providing feedback during the process, enhancing coherence and relevance. This approach moves beyond traditional methods of predicting words from static data.\n",
      "\n",
      "3. **Enhanced Decision-Making**: Combining RL with LLMs could enable better decision-making in dynamic environments, though this requires further exploration of how sequential decisions are handled by LLMs.\n",
      "\n",
      "4. **Scalability and Efficiency**: While RL can be resource-intensive, its application to LLMs might offer efficiency gains through improved learning from interactions rather than static data.\n",
      "\n",
      "5. **Reasoning Capabilities**: RL could enhance reasoning in LLMs, allowing them to generate more structured and coherent text and solve problems more effectively.\n",
      "\n",
      "6. **Challenges**: Key challenges include the computational demands of RL training, handling potential conflicts or errors in feedback, and ensuring ethical alignment to avoid negative outcomes.\n",
      "\n",
      "7. **Emerging Potential**: The use of on-policy methods that directly incorporate actions into training loops is promising. Meta-reinforcement learning could further enhance versatility by enabling LLMs to adapt to various tasks without extensive task-specific training.\n",
      "\n",
      "8. **Ethical Considerations**: Ensuring RL-driven LLMs are aligned with positive objectives is crucial to prevent biased or unintended behaviors, necessitating robust ethical frameworks.\n",
      "\n",
      "In conclusion, while RL presents significant opportunities for advancing LLM capabilities, particularly in interactive and dynamic contexts, it also poses challenges that require further research and consideration.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(user_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 15:08:24.078 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-29 15:08:24.102 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/drew_wilkins/Drews_Files/Drew/Python/Repositories/ASK/.venv-main/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-01-29 15:08:24.103 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-29 15:08:24.103 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-29 15:08:24.609 Thread 'Thread-5': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-29 15:08:24.609 Thread 'Thread-5': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-29 15:08:24.848 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-29 15:08:24.848 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context: 5 documents.\n",
      "LLM Response: content=\"To dispose of your old Operational Dress Uniforms (ODUs), you should follow these guidelines:\\n\\n1. **Uniform Condition**: Ensure that the ODUs are in a condition that is appropriate for disposal. If they are still serviceable, consider donating them to a local veterans' organization or a charity that accepts military uniforms.\\n\\n2. **Disposal Process**: If the ODUs are no longer usable, they should be disposed of in a manner that respects the uniform's significance. This can include:\\n   - Check with your local waste management for textile recycling options.\\n   - If the uniform is heavily worn or damaged, it can be disposed of with regular trash, but it is recommended to cut the uniforms to prevent misuse.\\n\\n3. **Regulations**: Be aware that the Coast Guard does not specify a formal process for uniform disposal in the provided regulations, but maintaining respect for the uniform's symbolism is encouraged. \\n\\n4. **Contact Local Units**: If you're unsure about how to dispose of your ODUs or if you want more specific guidance, you may contact your local Coast Guard unit or Auxiliary representative for advice.\\n\\nAlways ensure that any disposal method aligns with local regulations and environmental guidelines.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 241, 'prompt_tokens': 2386, 'total_tokens': 2627, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-4b16eb2b-a7c4-42b4-bcc2-8f850d33f413-0' usage_metadata={'input_tokens': 2386, 'output_tokens': 241, 'total_tokens': 2627, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Generate a response\n",
    "response = rag.rag(user_question)\n",
    "# response, enriched_question = rag.rag(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------- \\ ASK \\ ------------------------------------- \n",
      "\n",
      "USER QUESTION: How Do I Dispose of My Old ODUs?\n",
      "\n",
      "ENRICHED QUESTION: How Do I Dispose of My Old ODUs?\n",
      "\n",
      "RETRIEVED DOCUMENTS:\n",
      "Auxiliary Manual, COMDTINST M16790.1G, page 501\n",
      "UNINSPECTED PASSENGER VESSEL EXAMINATION PROGRAM  LANT/PACAREAINST 16710.2  Enclosure (1) Requirements For Uninspected Passenger Vessels , page 29\n",
      "ALAUX 021/23 OPERATION DRY WATER, page 2\n",
      "ALAUX 12/15 TEMPORARY SUSPENSION OF UDC Istore   12JUN2015, page 0\n",
      "CH-2 TO MARINE SAFETY MANUAL VOLUME II, MATERIEL INSPECTION, COMDTINST M16000.7B, page 1214\n",
      "\n",
      "ANSWER: To dispose of your old Operational Dress Uniforms (ODUs), you should follow these guidelines:\n",
      "\n",
      "1. **Uniform Condition**: Ensure that the ODUs are in a condition that is appropriate for disposal. If they are still serviceable, consider donating them to a local veterans' organization or a charity that accepts military uniforms.\n",
      "\n",
      "2. **Disposal Process**: If the ODUs are no longer usable, they should be disposed of in a manner that respects the uniform's significance. This can include:\n",
      "   - Check with your local waste management for textile recycling options.\n",
      "   - If the uniform is heavily worn or damaged, it can be disposed of with regular trash, but it is recommended to cut the uniforms to prevent misuse.\n",
      "\n",
      "3. **Regulations**: Be aware that the Coast Guard does not specify a formal process for uniform disposal in the provided regulations, but maintaining respect for the uniform's symbolism is encouraged. \n",
      "\n",
      "4. **Contact Local Units**: If you're unsure about how to dispose of your ODUs or if you want more specific guidance, you may contact your local Coast Guard unit or Auxiliary representative for advice.\n",
      "\n",
      "Always ensure that any disposal method aligns with local regulations and environmental guidelines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--------- \\\\ ASK \\ ------------------------------------- \\n\")\n",
    "print(f\"USER QUESTION: {response['user_question']}\\n\")\n",
    "print(f\"ENRICHED QUESTION: {response['enriched_question']}\\n\")\n",
    "print(\"RETRIEVED DOCUMENTS:\")\n",
    "print(\"\\n\".join(\n",
    "    map(lambda doc: f\"{doc.metadata['title']}, page {doc.metadata['page']}\", response['context'])))\n",
    "print(f\"\\nANSWER: {response['answer']}\\n\")\n",
    "# print(f\"LLM BASED ANSWER ON:\")\n",
    "# print(\"\\n\".join(response['llm_sources'])) # this is for AnswersWithSources\n",
    "\n",
    "\n",
    "# short_source_list = rag.create_short_source_list(response)\n",
    "# long_source_list = rag.create_long_source_list(response)\n",
    "# print(f\"Short source list: {short_source_list}\")\n",
    "# print(f\"{long_source_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(response[\"sources\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_text = response['answer']\n",
    "\n",
    "# Print the answer\n",
    "print(answer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(\n",
    "    map(lambda doc: f\"{doc.metadata['title']}, page {doc.metadata['page']}\", response['context'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and Response schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''{\n",
    "    'Question': 'string'\n",
    "    }\n",
    "\n",
    "response object:\n",
    "    {\n",
    "    'user_question': 'string'\n",
    "    'enriched_question': 'string'\n",
    "    'context': [list of Document objects]\n",
    "    'answer': {\n",
    "        'answer': 'string', \n",
    "        'llm_sources': ['string 1', 'string2']\n",
    "        }\n",
    "    }\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
