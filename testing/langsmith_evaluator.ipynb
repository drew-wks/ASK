{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an experiment in Langsmith\n",
    "### Do not add code to this to run a regular qa or it may put the wrong tracing project name. Use inference_tester.ipynb instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "load_dotenv('/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/.env')\n",
    "\n",
    "# Add the parent directory to sys.path so you can import your modules from a subdirectory\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import rag\n",
    "from rag import CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config LangSmith if you also want the traces\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain_evaluator.ipynb_on_ASK_main\"\n",
    "\n",
    "\n",
    "# ASK-groundtruth_v1  # one_example #ASK-groundtruth-BQII\n",
    "dataset_name = \"one_example\"\n",
    "# ASK-groundtruth_v1  ASK-accuracy-test-BQII_QA_set   initial_EDA\n",
    "experiment_prefix = \"testing the tracing\"\n",
    "\n",
    "experiment_description = \"\"\n",
    "# Checking accuracy of RAG on BQII_QA_set\n",
    "# Initial EDA check of dataset to ensure questions are in scope and groundtruth answers are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate for Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'testing the tracing-be67eac5' at:\n",
      "https://smith.langchain.com/o/3941ecea-6957-508c-9f4f-08ed62dc7d61/datasets/471f89ed-317b-4ed9-ad2e-38121ba9c2fa/compare?selectedSessions=bb07e473-bca0-47c2-abb2-39070d7c8cc4\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29ce50fe457435eb7053b19e36302ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "\n",
    "def prepare_cot_qa_data(run, example):\n",
    "    '''\n",
    "    Create a dictionary for the evaluator to use.\n",
    "    \n",
    "    run is the rag funtion \n",
    "    example is the example from the dataset\n",
    "    '''\n",
    "    return {\n",
    "        # Populates the input key with the question from the test dataset\n",
    "        \"input\": example.inputs[\"Question\"],\n",
    "        # Populates the reference key with the ground truth answer from the test dataset\n",
    "        \"reference\": example.outputs[\"Ground Truth Answer\"],\n",
    "        # Populates the prediction key with answer from rag.rag output\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "    }\n",
    "\n",
    "\n",
    "cot_qa_evaluator = LangChainStringEvaluator(\n",
    "    \"cot_qa\",\n",
    "    prepare_data=prepare_cot_qa_data\n",
    ")\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    # sends rag.rag a string instead of a dict, which is its default\n",
    "    lambda input: rag.rag(input[\"Question\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evaluator],\n",
    "    metadata=CONFIG,\n",
    "    experiment_prefix=experiment_prefix,\n",
    "    description=experiment_description,\n",
    "    # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate for Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-oai-hallucination-844ce45c' at:\n",
      "https://smith.langchain.com/o/3941ecea-6957-508c-9f4f-08ed62dc7d61/datasets/471f89ed-317b-4ed9-ad2e-38121ba9c2fa/compare?selectedSessions=0b15e093-28af-45ed-adbb-4b7c324a6658\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57c902a08d243449aab9b1dc0eaecf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "\n",
    "# Prompt to grade halucinations. It's a Langchain object with two inputs: \"documents\", \"student_answer\"\n",
    "grade_prompt_hallucinations = prompt = hub.pull(\n",
    "    \"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "\n",
    "def answer_hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for generation hallucination\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"Question\"]\n",
    "    contexts = run.outputs[\"context\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_hallucinations | llm\n",
    "\n",
    "    # Get score by passing populated prompt to the evaluator\n",
    "    score = answer_grader.invoke({\"documents\": contexts,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": score}\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    lambda input: rag.rag(input[\"Question\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_hallucination_evaluator],\n",
    "    experiment_prefix=\"rag-qa-oai-hallucination\",\n",
    "    metadata=CONFIG,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
