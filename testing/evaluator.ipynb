{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate RAG Quality\n",
    "##### Evaluates the app by running an experiment in Langsmith with the following metrics:\n",
    "-  Accuracy- Is the answer correct according to the ground truth answer\n",
    "-  Recall- How many of the relevant documents were retrieved\n",
    "-  Truthfulness - Did the response stray from the documents or hallucinate?\n",
    "\n",
    "Do not add code to this to run a regular rag inferences or it may put the wrong tracing project name. Use inference_tester.ipynb instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "import streamlit as st\n",
    "\n",
    "load_dotenv('/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/.env')\n",
    "\n",
    "# Add the parent directory to sys.path so you can import your modules from a subdirectory\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import rag\n",
    "from rag import CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config LangSmith if you also want the traces\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = st.secrets[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain_evaluator.ipynb on ASK main/local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from langsmith.evaluation import evaluate\n",
    "from langsmith import Client\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable\n",
    "\n",
    "# Set up logging\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "\n",
    "client = Client()\n",
    "\n",
    "eval_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt_accuracy = prompt = hub.pull(\n",
    "    \"cot_qa_drew\")\n",
    "\n",
    "\n",
    "def accuracy_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for detecting generation accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Inputs to Evaluator from Eval set\n",
    "    query = example.inputs[\"question\"]\n",
    "    ground_truth_answer = example.outputs[\"ground_truth_answer\"]\n",
    "\n",
    "    # Inputs to Evaluator from RAG output\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    llm = ChatOpenAI(model=eval_model, temperature=0, stream_usage=True)\n",
    "\n",
    "    # Define the grader\n",
    "    answer_grader = grade_prompt_accuracy | llm\n",
    "\n",
    "    # Get score by passing populated prompt to the evaluator\n",
    "    # The prompt template takes in \"query\", \"ground_truth_answer\", \"answer\" as inputs\n",
    "    grader_response = answer_grader.invoke({\"query\": query,\n",
    "                                           \"ground_truth_answer\": ground_truth_answer,\n",
    "                                            \"student_answer\": prediction}\n",
    "                                           )\n",
    "\n",
    "    correctness = grader_response[\"correctness\"]\n",
    "    explanation = grader_response[\"explanation\"]\n",
    "\n",
    "    return {\n",
    "        \"key\": \"Accuracy\",\n",
    "        \"score\": correctness,  # Numerical score expected by the evaluator\n",
    "        \"value\": \"Correct\" if correctness == 1 else \"Incorrect\",  # Optional categorical value\n",
    "        \"comment\": explanation,  # Additional metadata\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt_recall = prompt = hub.pull(\n",
    "    \"recall_drew\")\n",
    "\n",
    "\n",
    "def recall_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for checing the retrieved documents against the question\n",
    "    \"\"\"\n",
    "\n",
    "    # Inputs to Evaluator from Eval set\n",
    "    query = example.inputs[\"question\"]\n",
    "\n",
    "    # Inputs to Evaluator from RAG output\n",
    "    documents = run.outputs[\"context\"]\n",
    "    sources = run.outputs[\"sources\"]\n",
    "\n",
    "    # LLM grader\n",
    "    # other models gpt-4-turbo gpt-4o-mini\n",
    "    llm = ChatOpenAI(model=eval_model, temperature=0, stream_usage=True)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_recall | llm\n",
    "\n",
    "    # Get score by passing populated prompt to the evaluator\n",
    "    # The evaluator template expects \"documents\" as input\n",
    "    # The evaluator returns \"Score\" (int) and \"Explanation\" (str) as output\n",
    "    grader_response = answer_grader.invoke({\"query\": query,\n",
    "                                            \"documents\": documents})\n",
    "    score = grader_response[\"Score\"]\n",
    "    explanation = grader_response.get(\"Explanation\", \"No explanation provided\")\n",
    "\n",
    "    return {\"key\": \"Recall\", \"score\": score, \"sources\": sources, \"comment\": explanation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt_truthfulness = prompt = hub.pull(\n",
    "    \"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "\n",
    "def hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for detecting generation hallucinations\n",
    "    \"\"\"\n",
    "\n",
    "    # Inputs to Evaluator from Eval set\n",
    "    input_question = example.inputs[\"question\"]\n",
    "\n",
    "    # Inputs to Evaluator from RAG output\n",
    "    contexts = run.outputs[\"context\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    # other models gpt-4-turbo gpt-4o-mini\n",
    "    llm = ChatOpenAI(model=eval_model, temperature=0, stream_usage=True)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_truthfulness | llm\n",
    "\n",
    "    # Get score by passing populated prompt to the evaluator\n",
    "    # The evaluator template expects \"documents\" and \"student_answer\" as inputs\n",
    "    # The evaluator returns \"Score\" (int) and \"Explanation\" (str) as output\n",
    "    grader_response = answer_grader.invoke({\"documents\": contexts,\n",
    "                                            \"student_answer\": prediction})\n",
    "    score = grader_response[\"Score\"]\n",
    "    explanation = grader_response.get(\"Explanation\", \"No explanation provided\")\n",
    "\n",
    "    return {\"key\": \"Truthfulness\", \"score\": score, \"comment\": explanation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config your Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"ASK-groundtruth-v2\"\n",
    "# ASK-groundtruth_v1   initial_EDA\n",
    "\n",
    "data = dataset_name\n",
    "\n",
    "# I don't think I need this one anymore\n",
    "# data = client.list_examples(dataset_name=dataset_name, splits=[\"1_question\"])\n",
    "# data = client.list_examples(dataset_name=dataset_name, example_ids=[\n",
    "#                            \"2eea461c-3653-4c36-961f-256c70ee6268\"])\n",
    "\n",
    "# experiment_prefix = \"ASK_Eval_code_whichpromptisbroke\"\n",
    "experiment_prefix = \"ASK_ART_AnswerWithSources-gpt-4o-mini\"\n",
    "\n",
    "experiment_description = \"Baseline test with cleaner QA Eval Set. using gpt-4o-mini for Eval and gpt-4o-mini for RAG. \\n\\nNAMING CONVENTION\\nAppName_TestMetrics_TestVariables \\nExample: ASK_ART_llm-gpt-4o-mini\\nTest metrics are ART = Accuracy, Recall, Truthfulness. Test Variable is gpt-4o-mini which we will compare against some other llm. Other example of TestMetrics could be Eval_cost, App_cost, App_time, etc.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Evaluation\n",
    " OpenAI API pricing is [here.](https://openai.com/api/pricing/)  \n",
    " Your billing is [here.](https://platform.openai.com/settings/organization/usage/activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'ASK_ART_AnswerWithSources-gpt-4o-mini-5a95cd79' at:\n",
      "https://smith.langchain.com/o/3941ecea-6957-508c-9f4f-08ed62dc7d61/datasets/0b24ff94-f4f0-4197-89f3-765f835936c9/compare?selectedSessions=50bb176e-4dfa-438d-92f6-c679f3275553\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ae4edbd7a74f1f94f436d300599b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 19:25:33.320 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-15 19:25:33.339 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/drew_wilkins/Drews_Files/Drew/Python/Repositories/ASK/.venv-main/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-01-15 19:25:33.340 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-15 19:25:33.340 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-15 19:25:33.844 Thread 'Thread-10': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-15 19:25:33.845 Thread 'Thread-10': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-15 19:25:34.758 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-15 19:25:34.759 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.answer</th>\n",
       "      <th>outputs.sources</th>\n",
       "      <th>outputs.user_question</th>\n",
       "      <th>outputs.enriched_question</th>\n",
       "      <th>outputs.context</th>\n",
       "      <th>outputs.llm_sources</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.ground_truth_answer</th>\n",
       "      <th>reference.ground_truth_sources</th>\n",
       "      <th>feedback.Accuracy</th>\n",
       "      <th>feedback.Recall</th>\n",
       "      <th>feedback.Truthfulness</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the Auxiliary Chain of Leadership and ...</td>\n",
       "      <td>The Auxiliary Chain of Leadership and Manageme...</td>\n",
       "      <td>[AUXILIARY NATIONAL STAFF GUIDE, Auxiliary Man...</td>\n",
       "      <td>What is the Auxiliary Chain of Leadership and ...</td>\n",
       "      <td>What is the Auxiliary Chain of Leadership and ...</td>\n",
       "      <td>[page_content='19 The Auxiliary Chain of Leade...</td>\n",
       "      <td>[COMDTINST M16790.1G, Auxiliary Chain of Leade...</td>\n",
       "      <td>None</td>\n",
       "      <td>The Auxiliary Chain of Leadership and Manageme...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>12.068415</td>\n",
       "      <td>ab998c5d-bed3-41a5-9108-007848bd6658</td>\n",
       "      <td>4a9a08ad-2240-4948-939e-8e9e5b3e5535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Name one of the elected leader positions for t...</td>\n",
       "      <td>One of the elected leader positions for the Di...</td>\n",
       "      <td>[Auxiliary Manual, COMDTINST M16790.1G, Auxili...</td>\n",
       "      <td>Name one of the elected leader positions for t...</td>\n",
       "      <td>Name one of the elected leader positions for t...</td>\n",
       "      <td>[page_content='COMDTINST M16790.1G \\n \\n \\n \\n...</td>\n",
       "      <td>[COMDTINST M16790.1G]</td>\n",
       "      <td>None</td>\n",
       "      <td>One of the elected leader positions for the Di...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2.358701</td>\n",
       "      <td>a9a0b5b7-d729-4c81-8bf3-17b70782179a</td>\n",
       "      <td>3e0dcdd4-092b-4ba5-b53f-d45d056baf8e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Describe the purpose of the flotilla and revie...</td>\n",
       "      <td>The purpose of a flotilla within the Auxiliary...</td>\n",
       "      <td>[Auxiliary Flotilla Procedures Guide, Auxiliar...</td>\n",
       "      <td>Describe the purpose of the flotilla and revie...</td>\n",
       "      <td>Describe the purpose of the flotilla and revie...</td>\n",
       "      <td>[page_content='   \\n1-2 A. The Flotilla  \\nThe...</td>\n",
       "      <td>[Flotilla Administration Manual, Chapter 1, Fl...</td>\n",
       "      <td>None</td>\n",
       "      <td>The flotilla is the basic organizational unit ...</td>\n",
       "      <td>[Auxiliary Flotilla, Procedures Guide]</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9.816504</td>\n",
       "      <td>25a2dbd2-04b9-4e70-8cb4-c2b0cb1ad766</td>\n",
       "      <td>0b70a401-3b3f-4dda-bfb9-6ee43975b019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When walking abreast and overtaking a senior o...</td>\n",
       "      <td>When walking abreast and overtaking a senior o...</td>\n",
       "      <td>[Auxiliary Flotilla Procedures Guide, Navigati...</td>\n",
       "      <td>When walking abreast and overtaking a senior o...</td>\n",
       "      <td>When walking abreast and overtaking a senior o...</td>\n",
       "      <td>[page_content='   \\n6-5 National Ensign at the...</td>\n",
       "      <td>[6-5 National Ensign at the stern staff, B.2. ...</td>\n",
       "      <td>None</td>\n",
       "      <td>When walking and overtaking a senior, come abr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.174102</td>\n",
       "      <td>1f698eaa-02ed-466f-942b-9b466eb24f99</td>\n",
       "      <td>08a3130f-a15c-43b5-a0b0-66e69f8d3f93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the requirements to join the Auxiliary?</td>\n",
       "      <td>To join the United States Coast Guard Auxiliar...</td>\n",
       "      <td>[Auxiliary Flotilla Procedures Guide, Auxiliar...</td>\n",
       "      <td>What are the requirements to join the Auxiliary?</td>\n",
       "      <td>What are the requirements to join the Auxiliary?</td>\n",
       "      <td>[page_content='1-8 a.    Must, by the date of ...</td>\n",
       "      <td>[Auxiliary Training Handbook – Aviation, COMDT...</td>\n",
       "      <td>None</td>\n",
       "      <td>To join the Auxiliary, all of these requiremen...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4.806933</td>\n",
       "      <td>cb558ae2-d7b1-429d-954a-d0d2d9d7bd15</td>\n",
       "      <td>bf5e8908-41ba-42dd-91f1-5b2d3eca1651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Is the Coast Guard Mutual Assistance Program (...</td>\n",
       "      <td>Yes, the Coast Guard Mutual Assistance Program...</td>\n",
       "      <td>[AUXILIARY COAST GUARD MUTUAL ASSISTANCE AMBAS...</td>\n",
       "      <td>Is the Coast Guard Mutual Assistance Program (...</td>\n",
       "      <td>Is the Coast Guard Mutual Assistance Program (...</td>\n",
       "      <td>[page_content='United States Coast Guard Auxil...</td>\n",
       "      <td>[COMDTINST M16790.1G, Section F.2. Coast Guard...</td>\n",
       "      <td>None</td>\n",
       "      <td>Yes, the Coast Guard Mutual Assistance Program...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3.282748</td>\n",
       "      <td>f37eb9cd-2446-49d4-a139-87bb7c3f71b6</td>\n",
       "      <td>37ec53f8-6fa1-414b-bd37-77dee86f934f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Can an Auxiliarist use the Coast Guard Exchange?</td>\n",
       "      <td>Yes, Auxiliarists of appropriate age are autho...</td>\n",
       "      <td>[ALAUX 014/23 EXPANSION OF COAST GUARD EXCHANG...</td>\n",
       "      <td>Can an Auxiliarist use the Coast Guard Exchange?</td>\n",
       "      <td>Can an Auxiliarist use the Coast Guard Exchange?</td>\n",
       "      <td>[page_content='Page 1 of 1   \\n07 APR 2023  \\n...</td>\n",
       "      <td>[ALAUX 014/23]</td>\n",
       "      <td>None</td>\n",
       "      <td>Yes, Auxiliarists are authorized to use the Co...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3.016256</td>\n",
       "      <td>f4e77283-b478-4feb-ac59-4c1601e778a8</td>\n",
       "      <td>b7046d38-b3ee-441e-a8da-b7b7a4360c56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>On what occasion would you wear a Tropical Blu...</td>\n",
       "      <td>The Tropical Blue uniform is worn year-round o...</td>\n",
       "      <td>[Auxiliary Manual, COMDTINST M16790.1G, AUXILI...</td>\n",
       "      <td>On what occasion would you wear a Tropical Blu...</td>\n",
       "      <td>On what occasion would you wear a Tropical Blu...</td>\n",
       "      <td>[page_content='COMDTINST M16790.1G \\n \\n \\n \\n...</td>\n",
       "      <td>[COMDTINST M16790.1G, COMDTINST M1020.6K, AUX-...</td>\n",
       "      <td>None</td>\n",
       "      <td>The Tropical Blue Uniform is typically worn fo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5.227383</td>\n",
       "      <td>fd1632be-17f9-49a3-bf2c-83eb3693afb4</td>\n",
       "      <td>a166017f-c8d3-4438-92ea-9541e092f3d9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Who is eligible for flotilla elections?</td>\n",
       "      <td>To be eligible for flotilla elections, a candi...</td>\n",
       "      <td>[Auxiliary Manual, COMDTINST M16790.1G, Auxili...</td>\n",
       "      <td>Who is eligible for flotilla elections?</td>\n",
       "      <td>Who is eligible for flotilla elections?</td>\n",
       "      <td>[page_content='COMDTINST M16790.1G \\n \\n \\n \\n...</td>\n",
       "      <td>[COMDTINST M16790.1G]</td>\n",
       "      <td>None</td>\n",
       "      <td>Eligible Auxiliarists are those who meet the e...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9.260633</td>\n",
       "      <td>fddb8c31-7c76-4e32-922a-f1d6af258195</td>\n",
       "      <td>9790997b-2c02-4d55-aca2-3e784ce5ae09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Is it appropriate for junior officers to enter...</td>\n",
       "      <td>No, it is not appropriate for junior officers ...</td>\n",
       "      <td>[Auxiliary Flotilla Procedures Guide, AUXILIAR...</td>\n",
       "      <td>Is it appropriate for junior officers to enter...</td>\n",
       "      <td>Is it appropriate for junior officers to enter...</td>\n",
       "      <td>[page_content='6-16 appropriate title) has joi...</td>\n",
       "      <td>[COMDTINST M16790.1G, AUX- SOP-002(D)]</td>\n",
       "      <td>None</td>\n",
       "      <td>Yes, junior officers should enter boats and ve...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.778819</td>\n",
       "      <td>fff898fa-173d-4ebd-bdf6-707854bc207e</td>\n",
       "      <td>757f4164-a356-4bcf-a2f5-1b410ad08658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults ASK_ART_AnswerWithSources-gpt-4o-mini-5a95cd79>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    # maps the shape input from our example, which is a single-field dictionary, to the rag function we are testing, which accepts a string\n",
    "    lambda input: rag.rag(input[\"question\"]),\n",
    "    data=data,\n",
    "    # accuracy_evaluator, recall_evaluator, hallucination_evaluator\n",
    "    evaluators=[accuracy_evaluator, recall_evaluator, hallucination_evaluator],\n",
    "    experiment_prefix=experiment_prefix,\n",
    "    description=experiment_description,\n",
    "    num_repetitions=1,\n",
    "    metadata=CONFIG,\n",
    ")  # type: ignore    # This supresses an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
