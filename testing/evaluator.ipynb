{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate RAG Quality\n",
    "##### Evaluates the app by running an experiment in Langsmith  \n",
    "Do not add code to this to run a regular rag inferences or it may put the wrong tracing project name. Use inference_tester.ipynb instead\n",
    "\n",
    "Tests \n",
    "-  Accuracy (COT Answer Accuracy)\n",
    "-  Recall- How many of the relevant documents were retrieved\n",
    "-  Precision- How well did the response answer the question given the retrieved documents\n",
    "-  Truthfulness - Did the response stray from the documents or hallucinate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "load_dotenv('/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/.env')\n",
    "\n",
    "# Add the parent directory to sys.path so you can import your modules from a subdirectory\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import rag\n",
    "from rag import CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config LangSmith if you also want the traces\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain_evaluator.ipynb on ASK main/local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Accuracy Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def prepare_cot_qa_data(run, example):\n",
    "    '''\n",
    "    Create a dictionary for the evaluator to use.\n",
    "\n",
    "    run is the rag function \n",
    "    example is the example from the dataset\n",
    "    '''\n",
    "    return {\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "        \"reference\": example.outputs[\"ground_truth_answer\"],\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialize the LLM with the desired model and temperature\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# cot_qa uses the CotQAEvalChain class which uses the prompt template here: https://smith.langchain.com/hub/wfh/cot_qa\n",
    "accuracy_evaluator = LangChainStringEvaluator(\n",
    "    \"cot_qa\",\n",
    "    config={\"llm\": llm},\n",
    "    prepare_data=prepare_cot_qa_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Recall Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "\n",
    "\n",
    "recall_evaluator = LangChainStringEvaluator(\n",
    "    \"score_string\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"Recall\": \"\"\"The Assistant's Answer is a set of documents retrieved from a vectorstore. The input is a question used for retrieval. You will score whether the Assistant's Answer (retrieved documents) are relevant to the input question. The score should be between 0 and 10. A score of [[0]] means that the Assistant answer contains documents that are not at all relevant to the input question. A score of [[5]] menas that the Assistant answer contains some documents that are relevant to the input question. A score of [[10]] means that all of the Assistant answer documents are all relevant to the input question. If the user\\'s question is unclear or seems to be based on a misunderstanding or typo, you should give a score of [[0]].\"\"\",\n",
    "        },\n",
    "        \"normalize_by\": 10,\n",
    "        \"llm\": ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"context\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Truthfulness Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Prompt to grade Truthfulness. It's a Langchain object with two inputs: \"documents\", \"student_answer\"\n",
    "grade_prompt_truthfulness = prompt = hub.pull(\n",
    "    \"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "\n",
    "def hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for detecting generation hallucinations\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    contexts = run.outputs[\"context\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    # other models gpt-4-turbo gpt-4o-mini\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_truthfulness | llm\n",
    "\n",
    "    # Get score by passing populated prompt to the evaluator\n",
    "    score = answer_grader.invoke({\"documents\": contexts,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"Truthfulness\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config your Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = \"ASK-groundtruth-v2\"\n",
    "# ASK-groundtruth_v1   initial_EDA\n",
    "\n",
    "split_name = \"1_question\"\n",
    "\n",
    "data = dataset_name\n",
    "\n",
    "# I don't think I need this one anymore\n",
    "data = client.list_examples(dataset_name=dataset_name, splits=[\"1_question\"])\n",
    "\n",
    "\n",
    "experiment_prefix = \"ASK_ART_eval-llm-gpt-4o-mini\"\n",
    "\n",
    "experiment_description = \"Testing cost using gpt-4o-mini for Eval and gpt-4-turbo for RAG. This will run an eval over a signle question 1x. AppName-TestType-TestVariables. ART stands for Accuraacy, Recall, Truthfulness. oai= OpenAI model. accuracy, recall, truthfulness are the test variables.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Evaluation\n",
    " [OpenAI API pricing is here](https://openai.com/api/pricing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'ASK_ART_eval-llm-gpt-4o-mini-4bf178af' at:\n",
      "https://smith.langchain.com/o/3941ecea-6957-508c-9f4f-08ed62dc7d61/datasets/0b24ff94-f4f0-4197-89f3-765f835936c9/compare?selectedSessions=cb135756-eff8-4189-b5bd-e66472b0e76a\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d5ec1b35474b17b2dcfded5e278373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 19:12:05.171 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-12 19:12:05.188 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/drew_wilkins/Drews_Files/Drew/Python/Repositories/ASK/.venv-main/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-01-12 19:12:05.188 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-12 19:12:05.189 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-12 19:12:05.689 Thread 'Thread-8': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-12 19:12:05.690 Thread 'Thread-8': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-12 19:12:05.802 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-12 19:12:05.802 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.user_question</th>\n",
       "      <th>outputs.enriched_question</th>\n",
       "      <th>outputs.context</th>\n",
       "      <th>outputs.answer</th>\n",
       "      <th>outputs.llm_sources</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.ground_truth_answer</th>\n",
       "      <th>reference.ground_truth_sources</th>\n",
       "      <th>feedback.COT Contextual Accuracy</th>\n",
       "      <th>feedback.score_string:Recall</th>\n",
       "      <th>feedback.Truthfulness</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is harassment generally defined, and who i...</td>\n",
       "      <td>How is harassment generally defined, and who i...</td>\n",
       "      <td>How is harassment generally defined, and who i...</td>\n",
       "      <td>[page_content='COMDTINST M16790.1G \\n \\n \\n \\n...</td>\n",
       "      <td>Harassment is generally defined as unwelcome a...</td>\n",
       "      <td>[COMDTINST M16790.1G Section B Anti-Discrimina...</td>\n",
       "      <td>None</td>\n",
       "      <td>Harassment is generally defined as unwelcome a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.385029</td>\n",
       "      <td>a3326a11-5c24-4ceb-a04e-5bb708dd9b38</td>\n",
       "      <td>4548cb6d-68a0-41fd-be24-6fb030ca8e75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults ASK_ART_eval-llm-gpt-4o-mini-4bf178af>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def target_function(input: dict):\n",
    "    '''maps the shape input from our example, which is a single-field dictionary, to the rag function we are testing, which accepts a string'''\n",
    "    return rag.rag(input[\"question\"])\n",
    "\n",
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=data,\n",
    "    evaluators=[accuracy_evaluator, recall_evaluator,\n",
    "                hallucination_evaluator],\n",
    "    experiment_prefix=experiment_prefix,\n",
    "    num_repetitions=1,\n",
    "    metadata=CONFIG,\n",
    ")  # type: ignore    # This supresses an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
