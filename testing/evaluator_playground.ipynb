{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, SingleEvaluatorInput\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "def prepare_cot_qa_data(run, example):\n",
    "    '''\n",
    "    Create a dictionary for the evaluator to use.\n",
    "\n",
    "    run is the rag function \n",
    "    example is the example from the dataset\n",
    "    '''\n",
    "    return SingleEvaluatorInput(\n",
    "        input=example.inputs[\"question\"],\n",
    "        reference=example.outputs[\"ground_truth_answer\"],\n",
    "        prediction=run.outputs[\"answer\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize the LLM with the desired model and temperature\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "# cot_qa uses the CotQAEvalChain class which uses the prompt template here: https://smith.langchain.com/hub/wfh/cot_qa\n",
    "accuracy_evaluator = LangChainStringEvaluator(\n",
    "    \"cot_qa\",\n",
    "    prepare_data=prepare_cot_qa_data,\n",
    "    llm=llm  # Pass the LLM instance with the desired model and temperature\n",
    ")\n",
    "\n",
    "# Example usage of the evaluator\n",
    "input_data = {\n",
    "    \"question\": \"What is the capital of France?\",\n",
    "    \"ground_truth_answer\": \"Paris\",\n",
    "    \"answer\": \"Paris\"\n",
    "}\n",
    "\n",
    "# Simulate a run object\n",
    "run = {\n",
    "    \"outputs\": {\n",
    "        \"answer\": input_data[\"answer\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Simulate an example object\n",
    "example = {\n",
    "    \"inputs\": {\n",
    "        \"question\": input_data[\"question\"]\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"ground_truth_answer\": input_data[\"ground_truth_answer\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prepare the data for evaluation\n",
    "evaluator_input = prepare_cot_qa_data(run, example)\n",
    "\n",
    "# Evaluate the input data\n",
    "grader_response = accuracy_evaluator.evaluate(evaluator_input)\n",
    "\n",
    "# Print the grader response\n",
    "print(grader_response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
