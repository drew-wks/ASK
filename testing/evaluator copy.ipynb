{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "load_dotenv('/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/.env')\n",
    "\n",
    "# Add the parent directory to sys.path so you can import your modules from a subdirectory\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import rag\n",
    "from rag import CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config LangSmith if you also want the traces\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain_evaluator.ipynb on ASK main/local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def prepare_cot_qa_data(run, example):\n",
    "    '''\n",
    "    Create a dictionary for the evaluator to use.\n",
    "\n",
    "    run is the rag function \n",
    "    example is the example from the dataset\n",
    "    '''\n",
    "    return {\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "        \"reference\": example.outputs[\"ground_truth_answer\"],\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialize the LLM with the desired model and temperature\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "# cot_qa uses the CotQAEvalChain class which uses the prompt template here: https://smith.langchain.com/hub/wfh/cot_qa\n",
    "accuracy_evaluator = LangChainStringEvaluator(\n",
    "    \"cot_qa\",\n",
    "    prepare_data=prepare_cot_qa_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Grade output schema\n",
    "\n",
    "\n",
    "class CorrectnessGrade(TypedDict):\n",
    "    # Note that the order in the fields are defined is the order in which the model will generate them.\n",
    "    # It is useful to put explanations before responses because it forces the model to think through\n",
    "    # its final response before generating it:\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    correct: Annotated[bool, ...,\n",
    "                       \"True if the answer is correct, False otherwise.\"]\n",
    "\n",
    "\n",
    "# Grade prompt\n",
    "correctness_instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. \n",
    "(2) Ensure that the student answer does not contain any conflicting statements.\n",
    "(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.\n",
    "\n",
    "Correctness:\n",
    "A correctness value of True means that the student's answer meets all of the criteria.\n",
    "A correctness value of False means that the student's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader LLM\n",
    "grader_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\n",
    "    CorrectnessGrade, method=\"json_schema\", strict=True)\n",
    "\n",
    "\n",
    "def accuracy_evaluator(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"An evaluator for RAG answer accuracy\"\"\"\n",
    "    # Ensure keys exist in the inputs and outputs dictionaries\n",
    "    question = inputs.get(\"question\")\n",
    "    ground_truth_answer = inputs.get(\"ground_truth_answer\")\n",
    "    student_answer = outputs.get(\"answer\")\n",
    "\n",
    "    if not question or not ground_truth_answer or not student_answer:\n",
    "        raise ValueError(\n",
    "            \"Required input, reference output, or student answer is missing\")\n",
    "\n",
    "    answers = f\"\"\"      QUESTION: {inputs['question']}\n",
    "GROUND TRUTH ANSWER: {inputs['ground_truth_answer']}\n",
    "STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
    "\n",
    "    # Run evaluator\n",
    "    grade = grader_llm.invoke([{\"role\": \"system\", \"content\": correctness_instructions}, {\n",
    "                              \"role\": \"user\", \"content\": answers}])\n",
    "    return grade[\"correct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just testing what the AIMEssage looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "grade_prompt_accuracy = prompt = hub.pull(\n",
    "    \"cot_qa_drew\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "answer_grader = grade_prompt_accuracy | llm\n",
    "\n",
    "response = answer_grader.invoke({\"query\": \"What is the Auxiliary\",\n",
    "                                 \"context\": \"documents\",\n",
    "                                 \"result\": \"The Auxiliary is a rock band\"}\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "grade_prompt_accuracy = prompt = hub.pull(\n",
    "    \"wfh/cot_qa\")\n",
    "\n",
    "\n",
    "def accuracy_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for detecting generation accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input = example.inputs[\"question\"]\n",
    "    reference = example.outputs[\"ground_truth_answer\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    # other models gpt-4-turbo gpt-4o-mini\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_accuracy | llm\n",
    "\n",
    "    # Get score by passing populated prompt to the evaluator\n",
    "    score_response = answer_grader.invoke({\"query\": input,\n",
    "                                           \"context\": reference,\n",
    "                                           \"result\": prediction}\n",
    "                                          )\n",
    "    print(score_response)\n",
    "    reasoning = score_response.reasoning  # Accessing reasoning directly\n",
    "    value = score_response.value  # Accessing grade directly\n",
    "    score = score_response.score  # Accessing score directly\n",
    "\n",
    "    return {\"key\": \"Accuracy\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = \"ASK-groundtruth-v2\"\n",
    "# ASK-groundtruth_v1   initial_EDA\n",
    "\n",
    "split_name = \"1_question\"\n",
    "\n",
    "data = dataset_name\n",
    "\n",
    "# I don't think I need this one anymore\n",
    "data = client.list_examples(dataset_name=dataset_name, splits=[\"1_question\"])\n",
    "\n",
    "\n",
    "experiment_prefix = \"ASK_ART_eval-llm-gpt-4o-mini\"\n",
    "\n",
    "experiment_description = \"Testing cost using gpt-4o-mini for Eval and gpt-4-turbo for RAG. This will run an eval over a signle question 1x. AppName-TestType-TestVariables. ART stands for Accuraacy, Recall, Truthfulness. oai= OpenAI model. accuracy, recall, truthfulness are the test variables.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'ASK_ART_eval-llm-gpt-4o-mini-eab3607f' at:\n",
      "https://smith.langchain.com/o/3941ecea-6957-508c-9f4f-08ed62dc7d61/datasets/0b24ff94-f4f0-4197-89f3-765f835936c9/compare?selectedSessions=4226d0f7-e62b-4f98-95d8-3170e1a5faef\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d726b0c20fcc41078747ace69ea11540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.user_question</th>\n",
       "      <th>outputs.enriched_question</th>\n",
       "      <th>outputs.context</th>\n",
       "      <th>outputs.answer</th>\n",
       "      <th>outputs.llm_sources</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.ground_truth_answer</th>\n",
       "      <th>reference.ground_truth_sources</th>\n",
       "      <th>feedback.COT Contextual Accuracy</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is harassment generally defined, and who i...</td>\n",
       "      <td>How is harassment generally defined, and who i...</td>\n",
       "      <td>How is harassment generally defined, and who i...</td>\n",
       "      <td>[page_content='COMDTINST M16790.1G \\n \\n \\n \\n...</td>\n",
       "      <td>Harassment is generally defined as unwelcome a...</td>\n",
       "      <td>[COMDTINST M16790.1G Section B. Anti-Discrimin...</td>\n",
       "      <td>None</td>\n",
       "      <td>Harassment is generally defined as unwelcome a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>9.754796</td>\n",
       "      <td>a3326a11-5c24-4ceb-a04e-5bb708dd9b38</td>\n",
       "      <td>ab766cb2-b938-43d9-a57d-ab1a0adf0fef</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults ASK_ART_eval-llm-gpt-4o-mini-eab3607f>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def target_function(input: dict):\n",
    "    '''maps the shape input from our example, which is a single-field dictionary, to the rag function we are testing, which accepts a string'''\n",
    "    return rag.rag(input[\"question\"])\n",
    "\n",
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=data,\n",
    "    evaluators=[accuracy_evaluator],\n",
    "    experiment_prefix=experiment_prefix,\n",
    "    num_repetitions=1,\n",
    "    metadata=CONFIG,\n",
    ")  # type: ignore    # This supresses an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
