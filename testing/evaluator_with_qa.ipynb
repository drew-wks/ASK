{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate RAG Quality\n",
    "##### Evaluates the app by running an experiment in Langsmith with the following metrics:\n",
    "-  Accuracy- Is the answer correct according to the ground truth answer\n",
    "-  Recall- How many of the relevant documents were retrieved\n",
    "-  Truthfulness - Did the response stray from the documents or hallucinate?\n",
    "\n",
    "Do not add code to this to run a regular rag inferences or it may put the wrong tracing project name. Use inference_tester.ipynb instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "import streamlit as st\n",
    "\n",
    "load_dotenv(\n",
    "    '/Users/drew_wilkins/Drews_Files/Drew/Python/Localcode/.env', override=True)\n",
    "\n",
    "\n",
    "# Add the parent directory to sys.path so you can import your modules from a subdirectory\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import rag\n",
    "from rag import CONFIG\n",
    "\n",
    "from langsmith.evaluation import evaluate\n",
    "from langsmith import Client\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable\n",
    "from langsmith.utils import ContextThreadPoolExecutor\n",
    "from concurrent.futures import TimeoutError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **OPTIONAL:** Recored traces of rag. Required for cost and token info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config LangSmith observability if you want to see the traces for this notebook\n",
    "# This assumes you have any traces left in your monthly usage allotment LOL!\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = st.secrets[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"inference_tester.ipynb_on_ASK_main\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select your LangSmith account based on API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the Langsmith account you want to use based on the API key\n",
    "client = Client(api_key=os.environ[\"LANGCHAIN_API_KEY\"])\n",
    "\n",
    "eval_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **OPTIONAL:** Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(\"langsmith\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def validate_and_fix_json(raw_output: Any, required_fields: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validates and fixes JSON output, ensuring required fields are present.\n",
    "\n",
    "    Args:\n",
    "        raw_output (Any): The raw JSON string or dictionary from the LLM.\n",
    "        required_fields (dict): A dictionary of required fields with their default values.\n",
    "\n",
    "    Returns:\n",
    "        dict: Validated and fixed JSON output aligned with required fields.\n",
    "    \"\"\"\n",
    "    # If the input is already a dictionary, skip parsing\n",
    "    if isinstance(raw_output, dict):\n",
    "        parsed_response = raw_output\n",
    "    else:\n",
    "        try:\n",
    "            # Attempt to parse the JSON string\n",
    "            parsed_response = json.loads(raw_output)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSONDecodeError: {e}\")\n",
    "            print(f\"Raw output: {raw_output}\")\n",
    "\n",
    "            # Attempt common fixes\n",
    "            if isinstance(raw_output, str):\n",
    "                if raw_output.strip().endswith('\"'):\n",
    "                    raw_output = raw_output.rstrip('\"') + '\"}'\n",
    "                elif not raw_output.strip().endswith('}'):\n",
    "                    raw_output += '}'\n",
    "\n",
    "                # Retry parsing\n",
    "                try:\n",
    "                    parsed_response = json.loads(raw_output)\n",
    "                except json.JSONDecodeError as final_e:\n",
    "                    print(f\"Failed to fix JSON: {final_e}\")\n",
    "                    parsed_response = {}\n",
    "            else:\n",
    "                # If it's not a string and cannot be parsed, fallback to empty dict\n",
    "                parsed_response = {}\n",
    "\n",
    "    # Ensure required fields are present with default values\n",
    "    validated_response = {\n",
    "        key: parsed_response.get(key, default) for key, default in required_fields.items()\n",
    "    }\n",
    "\n",
    "    return validated_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.utils import ContextThreadPoolExecutor\n",
    "from concurrent.futures import TimeoutError\n",
    "\n",
    "\n",
    "def invoke_with_timeout(grader, input_data, timeout=60):\n",
    "    \"\"\"\n",
    "    Invokes an LLM grader with a timeout.\n",
    "\n",
    "    Args:\n",
    "        grader: The structured LangChain prompt + LLM model.\n",
    "        input_data (dict): The input dictionary for the LLM grader.\n",
    "        timeout (int): The maximum time to wait before returning defaults.\n",
    "\n",
    "    Returns:\n",
    "        dict: The validated response from the grader or default values.\n",
    "    \"\"\"\n",
    "\n",
    "    def invoke_grader():\n",
    "        return grader.invoke(input_data)\n",
    "\n",
    "    with ContextThreadPoolExecutor() as executor:\n",
    "        future = executor.submit(invoke_grader)\n",
    "        try:\n",
    "            return future.result(timeout=timeout)\n",
    "        except TimeoutError:\n",
    "            print(\"LLM grader call timed out. Cancelling future and returning None.\")\n",
    "            future.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt_accuracy = prompt = hub.pull(\n",
    "    \"drew-wks/cot_qa\")\n",
    "\n",
    "# https://smith.langchain.com/hub/drew-wks/cot_qa?organizationId=adac21b1-016d-49e4-84f0-672bf1a6e7b1\n",
    "\n",
    "\n",
    "def accuracy_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for detecting generation accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Inputs to Evaluator from Eval set\n",
    "    query = example.inputs[\"question\"]\n",
    "    print(f\"Accuracy eval: retrieving result for question: {query}\")\n",
    "    ground_truth_answer = example.outputs[\"ground_truth_answer\"]\n",
    "\n",
    "    # Inputs to Evaluator from RAG output\n",
    "    prediction = run.outputs.get(\"answer\")\n",
    "    if prediction is None:\n",
    "        print(f\"'answer' key is missing in outputs: {run.outputs}\")\n",
    "\n",
    "    llm = ChatOpenAI(model=eval_model, temperature=0,\n",
    "                     tags=[\"accuracy_evaluator\"])\n",
    "\n",
    "    # Define the grader\n",
    "    answer_grader = grade_prompt_accuracy | llm\n",
    "\n",
    "    # Get score by passing populated prompt to the evaluator\n",
    "    # The prompt template takes in \"query\", \"ground_truth_answer\", \"answer\" as inputs\n",
    "    print(\"Passing result to LLM to grade accuracy\")\n",
    "    grader_response = invoke_with_timeout(answer_grader, {\n",
    "        \"query\": query,\n",
    "        \"ground_truth_answer\": ground_truth_answer,\n",
    "        \"student_answer\": prediction\n",
    "    })\n",
    "\n",
    "    if grader_response is None:\n",
    "        return {\n",
    "            \"key\": \"Accuracy\",\n",
    "            \"score\": 0,\n",
    "            \"comment\": \"LLM evaluation timed out.\"\n",
    "        }\n",
    "\n",
    "    required_fields = {\n",
    "        \"correctness\": 0,  # Default correctness value\n",
    "        \"explanation\": \"No explanation provided.\"  # Default explanation\n",
    "    }\n",
    "\n",
    "    validated_response = validate_and_fix_json(\n",
    "        grader_response, required_fields)\n",
    "\n",
    "    correctness = validated_response[\"correctness\"]\n",
    "    explanation = validated_response[\"explanation\"]\n",
    "\n",
    "    return {\n",
    "        \"key\": \"Accuracy\",\n",
    "        \"score\": correctness,  # Numerical score expected by the evaluator\n",
    "        \"comment\": explanation,  # Additional metadata\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt_recall = prompt = hub.pull(\n",
    "    \"drew-wks/recall_drew\")\n",
    "\n",
    "\n",
    "def recall_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for checing the retrieved documents against the question\n",
    "    \"\"\"\n",
    "\n",
    "    # Inputs to Evaluator from Eval set\n",
    "    query = example.inputs[\"question\"]\n",
    "    print(f\"Recall eval: retrieving docs for question: {query}\")\n",
    "\n",
    "    # Inputs to Evaluator from RAG output\n",
    "    documents = run.outputs.get(\"context\")\n",
    "    if documents is None:\n",
    "        print(f\"'context' key is missing in outputs: {run.outputs}\")\n",
    "    sources = run.outputs.get(\"sources\")\n",
    "    if sources is None:\n",
    "        print(f\"'sources' key is missing in outputs: {run.outputs}\")\n",
    "\n",
    "    # LLM grader\n",
    "    # other models gpt-4-turbo gpt-4o-mini\n",
    "    llm = ChatOpenAI(model=eval_model, temperature=0,\n",
    "                     tags=[\"recall_evaluator\"])\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_recall | llm\n",
    "\n",
    "    # Get score by passing populated prompt to the evaluator\n",
    "    # The evaluator template expects \"documents\" as input\n",
    "    # The evaluator returns \"Score\" (int) and \"Explanation\" (str) as output\n",
    "    print(\"Passing result to LLM to grade recall\")\n",
    "    grader_response = invoke_with_timeout(answer_grader, {\n",
    "        \"query\": query,\n",
    "        \"documents\": documents\n",
    "    })\n",
    "\n",
    "    if grader_response is None:\n",
    "        return {\n",
    "            \"key\": \"Recall\",\n",
    "            \"score\": 0,\n",
    "            \"sources\": sources,\n",
    "            \"comment\": \"LLM evaluation timed out.\"\n",
    "        }\n",
    "\n",
    "    required_fields = {\n",
    "        \"Score\": 0,  # Default value\n",
    "        \"Explanation\": \"No explanation provided.\"  # Default value\n",
    "    }\n",
    "\n",
    "    validated_response = validate_and_fix_json(\n",
    "        grader_response, required_fields)\n",
    "\n",
    "    score = validated_response[\"Score\"]\n",
    "    explanation = validated_response[\"Explanation\"]\n",
    "\n",
    "    return {\"key\": \"Recall\", \"score\": score, \"sources\": sources, \"comment\": explanation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt_truthfulness = prompt = hub.pull(\n",
    "    \"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "\n",
    "def hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for detecting generation hallucinations\n",
    "    \"\"\"\n",
    "\n",
    "    # Inputs to Evaluator from Eval set\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    print(\n",
    "        f\"Truthfulness eval: retrieving result for question: {input_question}\")\n",
    "\n",
    "    # Inputs to Evaluator from RAG output\n",
    "    documents = run.outputs.get(\"context\")\n",
    "    if not documents:\n",
    "        print(\n",
    "            f\"No documents retrieved. Skipping grading. Outputs: {run.outputs}\")\n",
    "        return {\n",
    "            \"key\": \"Truthfulness\",\n",
    "            \"score\": 0,  # Or any default score you'd prefer for empty context\n",
    "            \"comment\": \"No relevant documents were found to evaluate the answer.\"\n",
    "        }\n",
    "\n",
    "    prediction = run.outputs.get(\"answer\")\n",
    "    if not prediction:\n",
    "        print(f\"'answer' key is missing in outputs: {run.outputs}\")\n",
    "        return {\n",
    "            \"key\": \"Truthfulness\",\n",
    "            \"score\": 0,\n",
    "            \"comment\": \"No answer provided to evaluate.\"\n",
    "        }\n",
    "    print(\"Passing result to LLM to grade truth\")\n",
    "    # LLM grader\n",
    "    # other models gpt-4-turbo gpt-4o-mini\n",
    "    llm = ChatOpenAI(model=eval_model, temperature=0,\n",
    "                     tags=[\"hallucination_evaluator\"])\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_truthfulness | llm\n",
    "\n",
    "    # Get score by passing populated prompt to the evaluator\n",
    "    # The evaluator template expects \"documents\" and \"student_answer\" as inputs\n",
    "    # The evaluator returns \"Score\" (int) and \"Explanation\" (str) as output\n",
    "    grader_response = invoke_with_timeout(answer_grader, {\n",
    "        \"documents\": documents,\n",
    "        \"student_answer\": prediction\n",
    "    })\n",
    "\n",
    "    if grader_response is None:\n",
    "        return {\n",
    "            \"key\": \"Truthfulness\",\n",
    "            \"score\": 0,\n",
    "            \"comment\": \"LLM evaluation timed out.\"\n",
    "        }\n",
    "\n",
    "    required_fields = {\n",
    "        \"Score\": None,  # Default value\n",
    "        \"Explanation\": \"No explanation provided.\"  # Default value\n",
    "    }\n",
    "\n",
    "    validated_response = validate_and_fix_json(\n",
    "        grader_response, required_fields)\n",
    "\n",
    "    score = validated_response[\"Score\"]\n",
    "    explanation = validated_response[\"Explanation\"]\n",
    "\n",
    "    return {\"key\": \"Truthfulness\", \"score\": score, \"comment\": explanation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1:** Use Lagnsmith dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"ASK-groundtruth-v3\"\n",
    "# ASK-groundtruth-v3 ASK-groundtruth_v1   initial_EDA one_example_easy\n",
    "\n",
    "data = dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2:** Use single Lagnsmith example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't think I need this one anymore\n",
    "# data = client.list_examples(dataset_name=dataset_name, splits=[\"1_question\"])\n",
    "# data = client.list_examples(dataset_name=dataset_name, example_ids=[\n",
    "#                            \"a3326a11-5c24-4ceb-a04e-5bb708dd9b38\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3:** Use JSONL file\n",
    "\n",
    "NOTE: Be sure to set evaluate.upload_results=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# one_example  dataset_0b24ff94-f4f0-4197-89f3-765f835936c9\n",
    "examples_file_path = \"one_example.jsonl\"\n",
    "\n",
    "\n",
    "# Convert JSONL data to `schemas.Example` objects\n",
    "with open(examples_file_path, \"r\") as f:\n",
    "    data = [\n",
    "        Example(\n",
    "            id=str(uuid.uuid4()),\n",
    "            inputs={\"question\": entry[\"inputs\"][\"question\"]},\n",
    "            outputs={\n",
    "                # Ground truth answer\n",
    "                \"ground_truth_answer\": entry[\"outputs\"][\"ground_truth_answer\"],\n",
    "                # Ground truth sources\n",
    "                \"ground_truth_sources\": entry[\"outputs\"][\"ground_truth_sources\"]\n",
    "            },\n",
    "            metadata={\n",
    "                # Dataset split information\n",
    "                \"dataset_split\": entry[\"metadata\"][\"dataset_split\"]\n",
    "            }\n",
    "        )\n",
    "        for entry in map(json.loads, f)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_prefix = \"ASK_Eval_code_whichpromptisbroke\"\n",
    "experiment_prefix = \"ASK_AT_without-AnswersWithSources-prompt\"\n",
    "\n",
    "experiment_description = \"Testing the rag on full set without AnswersWithSources using gpt-4o-mini. \\n\\nNAMING CONVENTION\\nAppName_TestMetrics_TestVariables \\nExample: ASK_ART_llm-gpt-4o-mini\\nTest metrics are CLART = Cost, Latency, Accuracy, Recall, Truthfulness. Test Variable is gpt-4o-mini which we will compare against some other llm. Other example of TestMetrics could be Eval_cost, App_cost, App_time, etc.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Evaluation\n",
    " OpenAI API pricing is [here.](https://openai.com/api/pricing/)  \n",
    " Your billing is [here.](https://platform.openai.com/settings/organization/usage/activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'ASK_AT_without-AnswersWithSources-prompt-02b627da' at:\n",
      "https://smith.langchain.com/o/adac21b1-016d-49e4-84f0-672bf1a6e7b1/datasets/e44c3abc-7871-49a3-a388-6197d7c2dcf3/compare?selectedSessions=5a505541-37aa-46d7-95b7-431eb31e99ae\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0a284833b54affae48801f1ff17b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Is it appropriate for junior officers to enter boats and vehicles first?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Is it appropriate for junior officers to enter boats and vehicles first?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Who is eligible for flotilla elections?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Who is eligible for flotilla elections?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: On what occasion would you wear a Tropical Blue Uniform?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: On what occasion would you wear a Tropical Blue Uniform?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Can an Auxiliarist use the Coast Guard Exchange?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Can an Auxiliarist use the Coast Guard Exchange?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Is the Coast Guard Mutual Assistance Program (CGMA) available to Auxiliarists?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Is the Coast Guard Mutual Assistance Program (CGMA) available to Auxiliarists?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: When can Auxiliarists represent the Auxiliary?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: When can Auxiliarists represent the Auxiliary?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: How often does an Auxiliary Unit need to conduct an inventory and to whom should they report it?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: How often does an Auxiliary Unit need to conduct an inventory and to whom should they report it?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: On what occasion would you wear an ODU?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: On what occasion would you wear an ODU?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: When do members of the Auxiliary have the same power and authority in execution of duties as members of the Coast Guard?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: When do members of the Auxiliary have the same power and authority in execution of duties as members of the Coast Guard?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: What is AUXDATA?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: What is AUXDATA?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Describe how to properly render a salute to the colors.\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: Describe how to properly render a salute to the colors.\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Who can be reimbursed expenses of operation, maintenance and repair or replacement of personal property of the Auxiliary?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Who can be reimbursed expenses of operation, maintenance and repair or replacement of personal property of the Auxiliary?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Which administrative level(s) hold elections?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Which administrative level(s) hold elections?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Define what Personally Identifiable Information (PII) is.\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Define what Personally Identifiable Information (PII) is.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: How are CG enlisted personnel addressed?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: How are CG enlisted personnel addressed?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Describe the expected procedure when a senior officer enters a space occupied only by junior officers.\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Describe the expected procedure when a senior officer enters a space occupied only by junior officers.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What agencies help promote VSCs and PE Activities?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What agencies help promote VSCs and PE Activities?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: When an Auxiliarist is assigned to duty and traveling, what are some of the allowed reimbursable expenses?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: When an Auxiliarist is assigned to duty and traveling, what are some of the allowed reimbursable expenses?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: How are commissioned officers addressed?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: How are commissioned officers addressed?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What are the Core Values?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What are the Core Values?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Name 3 Operational Training Programs.\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Name 3 Operational Training Programs.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Are expenses associated with operating a Facility reimbursable?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Are expenses associated with operating a Facility reimbursable?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: When was the Coast Guard Auxiliary established?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: When was the Coast Guard Auxiliary established?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What type of mail can be mailed as business mail?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What type of mail can be mailed as business mail?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Medals and awards will be worn as prescribed where?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Medals and awards will be worn as prescribed where?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Is the Coast Guard Auxiliary a military organization?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Is the Coast Guard Auxiliary a military organization?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Define 'gender discrimination'.\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Define 'gender discrimination'.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Name three (3) programs/activities authorized for Auxiliarists.\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Name three (3) programs/activities authorized for Auxiliarists.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Explain what can and cannot be done with social media.\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Explain what can and cannot be done with social media.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Does the Auxiliary have a published diversity policy statement, and if so, where can it be found?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Does the Auxiliary have a published diversity policy statement, and if so, where can it be found?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: What is the Auxiliary Center (AUXCEN)?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: What is the Auxiliary Center (AUXCEN)?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Review and discuss Auxiliary participation in funeral services and notification of the appropriate individual regarding the death of an Auxiliarist.\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Review and discuss Auxiliary participation in funeral services and notification of the appropriate individual regarding the death of an Auxiliarist.\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Where can you find information about proper courtesy for correspondence?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Where can you find information about proper courtesy for correspondence?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Who prescribes the Uniform of the Day?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Who prescribes the Uniform of the Day?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What is the Coast Guard policy on Rape and Sexual Assault?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What is the Coast Guard policy on Rape and Sexual Assault?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What does 'personal property of the Auxiliary' mean?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What does 'personal property of the Auxiliary' mean?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What are the four cornerstones?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: What are the four cornerstones?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: When shall ethical standards be applied?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: When shall ethical standards be applied?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Auxiliarists participating in Auxiliary Marketing and Public Affairs program shall direct their efforts toward publicizing what?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Auxiliarists participating in Auxiliary Marketing and Public Affairs program shall direct their efforts toward publicizing what?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What rights, privileges, powers, or duties do Auxiliarists have?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: What rights, privileges, powers, or duties do Auxiliarists have?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What types of Orders are reimbursable?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What types of Orders are reimbursable?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What is Form 7029 used for?\n",
      "Retrieved context: 5 documents.\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What is Form 7029 used for?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: How often are flotilla elections held?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: How often are flotilla elections held?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: What is a Member Involvement Plan and where can it be found?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What is a Member Involvement Plan and where can it be found?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Review the eligibility requirements to become an Auxiliarist.\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response receivedTruthfulness eval: retrieving result for question: Review the eligibility requirements to become an Auxiliarist.\n",
      "Passing result to LLM to grade truth\n",
      "\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Can Coast Guard property be used for private use by Auxiliarists?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: Can Coast Guard property be used for private use by Auxiliarists?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: What is the Coast Guard motto and what does it mean?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: What is the Coast Guard motto and what does it mean?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What is the difference between Auxiliary and active duty insignia?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What is the difference between Auxiliary and active duty insignia?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: When is membership achieved?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: When is membership achieved?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Are all members issued an Auxiliary Member Identification Card?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Are all members issued an Auxiliary Member Identification Card?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: How many course credits are required to become AUXOP qualified?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: How many course credits are required to become AUXOP qualified?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Discuss the significance of the salute and when a salute should be rendered.\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Discuss the significance of the salute and when a salute should be rendered.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What is meant by 'Fair treatment'?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What is meant by 'Fair treatment'?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Is there a requirement to be a member of a specific flotilla?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Is there a requirement to be a member of a specific flotilla?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What are the Flotilla Standing Rules?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What are the Flotilla Standing Rules?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: At what level within the AUX organization should sexual harassment issues be resolved?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: At what level within the AUX organization should sexual harassment issues be resolved?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: What is the key to preventing harassment?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: What is the key to preventing harassment?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What are Memorandums of Understanding (MOU) and Memorandums of Agreement (MOA)?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What are Memorandums of Understanding (MOU) and Memorandums of Agreement (MOA)?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: What shirt is worn with the Service Dress Blue Uniform?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: What shirt is worn with the Service Dress Blue Uniform?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What is the difference between active duty personnel and Auxiliary members?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What is the difference between active duty personnel and Auxiliary members?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Define what a Personnel Security Investigation (PSI) is and its purpose.\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Define what a Personnel Security Investigation (PSI) is and its purpose.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Explain what needs to be done when an Auxiliarist is injured when assigned to duty.\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Explain what needs to be done when an Auxiliarist is injured when assigned to duty.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What is the Auxiliary National Supply Center (ANSC)?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What is the Auxiliary National Supply Center (ANSC)?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Can an Auxiliarist communicate as a private citizen with elected and appointed government officials?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Can an Auxiliarist communicate as a private citizen with elected and appointed government officials?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: What materials are supplied by the CGAuxA?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: What materials are supplied by the CGAuxA?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Can unreimbursed out-of-pocket expenses be deducted from personal income tax returns?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Can unreimbursed out-of-pocket expenses be deducted from personal income tax returns?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: What documentation does a former U.S. military person need to show in order to wear U.S. military awards received?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: What documentation does a former U.S. military person need to show in order to wear U.S. military awards received?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: When communicating verbally, how do you address Admirals, Captains, Commanders, and Commodores?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: When communicating verbally, how do you address Admirals, Captains, Commanders, and Commodores?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Name one of the elected leader positions for the Division.\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Name one of the elected leader positions for the Division.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Describe the function of the National Executive Committee (NEXCOM).\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Describe the function of the National Executive Committee (NEXCOM).\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: How shall the treatment and handling of PII be done?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: How shall the treatment and handling of PII be done?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What is the purpose of Auxiliary Administrative Discipline?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What is the purpose of Auxiliary Administrative Discipline?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What is an elected leader?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What is an elected leader?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Are Auxiliarists in AP status authorized to obtain and wear the Auxiliary uniform with proper devices and insignia of the current or highest past office held?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Are Auxiliarists in AP status authorized to obtain and wear the Auxiliary uniform with proper devices and insignia of the current or highest past office held?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Does the Auxiliary have a published anti-discrimination and anti-harassment policy statement, and if so, where can it be found?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Does the Auxiliary have a published anti-discrimination and anti-harassment policy statement, and if so, where can it be found?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Can an Auxiliarist be compensated if injured while assigned to duty?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Can an Auxiliarist be compensated if injured while assigned to duty?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Discuss the physical fitness requirements associated with being an Auxiliarist.\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Discuss the physical fitness requirements associated with being an Auxiliarist.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What is the purpose of an Auxiliary district board?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What is the purpose of an Auxiliary district board?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Explain what 'assignment to duty' is and who assigns it.\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Explain what 'assignment to duty' is and who assigns it.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: How are warrant officers addressed?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: How are warrant officers addressed?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What is the difference between USCG active duty commissioned officers, warrant officers, and enlisted personnel?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What is the difference between USCG active duty commissioned officers, warrant officers, and enlisted personnel?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Auxiliarists in AP status are authorized to do what?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Auxiliarists in AP status are authorized to do what?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Must an Auxiliarist have DD/EFT to be reimbursed expenses?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Must an Auxiliarist have DD/EFT to be reimbursed expenses?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What is the role of the Auxiliary?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: What is the role of the Auxiliary?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Can Auxiliary rosters be used for non-auxiliary purposes?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Can Auxiliary rosters be used for non-auxiliary purposes?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Who is responsible for providing overall leadership and direction to ensure all policies and procedures contained in this section are in effect throughout the Coast Guard and Auxiliary?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: Who is responsible for providing overall leadership and direction to ensure all policies and procedures contained in this section are in effect throughout the Coast Guard and Auxiliary?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Who administers the Auxiliary?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Who administers the Auxiliary?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Describe the differences between AP, IQ, BQ, and AX/AX2 status.\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Describe the differences between AP, IQ, BQ, and AX/AX2 status.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: State the difference between the active duty and the Auxiliary grooming and personal appearance standards?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: State the difference between the active duty and the Auxiliary grooming and personal appearance standards?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Do Aviators require TCT training to maintain their qualifications? Is the CRM class they are required to take equivalent to TCT? In other words, do pilots and observers have to take TCT and the one hour refresher? If they are both a Surface Operator and an Aviator, do they have to take both TCT and CRM, or will one substitute for the other?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Do Aviators require TCT training to maintain their qualifications? Is the CRM class they are required to take equivalent to TCT? In other words, do pilots and observers have to take TCT and the one hour refresher? If they are both a Surface Operator and an Aviator, do they have to take both TCT and CRM, or will one substitute for the other?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Can I use a flashing yellow light when towing a disabled vessel?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Can I use a flashing yellow light when towing a disabled vessel?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Will a member's hours underway, as crew aboard Coast Guard vessels on patrol, and reported on Form 7030 as Mission 26, 'Coast Guard Crew Augmentation', as Non-Lead,  be counted toward a Coxswains annual currency requirements?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Will a member's hours underway, as crew aboard Coast Guard vessels on patrol, and reported on Form 7030 as Mission 26, 'Coast Guard Crew Augmentation', as Non-Lead,  be counted toward a Coxswains annual currency requirements?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: If an Auxiliarist in AP status is arrested, even if not while assigned to duty, must they notify the DIRAUX?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: If an Auxiliarist in AP status is arrested, even if not while assigned to duty, must they notify the DIRAUX?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Is an Auxiliarist in AP status counted on unit rosters?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Is an Auxiliarist in AP status counted on unit rosters?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: What are the steps to membership in the Coast Guard Auxiliary?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What are the steps to membership in the Coast Guard Auxiliary?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Are AUXDATA entries made for Auxiliarists in AP Status?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Are AUXDATA entries made for Auxiliarists in AP Status?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What do I need to do to remain qualified?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What do I need to do to remain qualified?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Where can I find the Oath of Membership for new Auxiliary members?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Where can I find the Oath of Membership for new Auxiliary members?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What do the three types of PSIs represent?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What do the three types of PSIs represent?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Where do Kayaks and Canoes fit into the Navigation Rules\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Where do Kayaks and Canoes fit into the Navigation Rules\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Can a FC from another flotilla do my uniform inspection?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Can a FC from another flotilla do my uniform inspection?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: When is an applicant officially recognized as an Auxiliarist?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: When is an applicant officially recognized as an Auxiliarist?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Can we have Spiritual Elements at Auxiliary Functions?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Can we have Spiritual Elements at Auxiliary Functions?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: How do I help out at after a hurricane?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: How do I help out at after a hurricane?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What \"vessels\" are required to comply with the Navigation Rules?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What \"vessels\" are required to comply with the Navigation Rules?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: How many fatigue hours for a patrol where I was underway for 7 hours and then stopped for lunch for an hour and then we spent an combined hour on the boat before and after the patrol\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: How many fatigue hours for a patrol where I was underway for 7 hours and then stopped for lunch for an hour and then we spent an combined hour on the boat before and after the patrol\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Where Should the VSC Decal Be Placed?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Where Should the VSC Decal Be Placed?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: What Can I Do While I am in AP Status?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: What Can I Do While I am in AP Status?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What are the types of PSIs?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What are the types of PSIs?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Do Women Have To Wear a T-Shirt With The Tropical Blue Uniform?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Do Women Have To Wear a T-Shirt With The Tropical Blue Uniform?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: How can I volunteer for emergency communications during a national disaster?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: How can I volunteer for emergency communications during a national disaster?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Are Auxiliarists in AP status authorized to receive Auxiliary publications?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Are Auxiliarists in AP status authorized to receive Auxiliary publications?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: I am a Coxswain and own a vessel, may I take non-operationally qualified members who want to do ATON work on patrol with me?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: I am a Coxswain and own a vessel, may I take non-operationally qualified members who want to do ATON work on patrol with me?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Who is required to complete BQC II?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Who is required to complete BQC II?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: While on a patrol an OPFAC comes upon a vessel which has run aground. After assessing the risk to both vessels may the facility be used to tow the stranded vessel back into safe water?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: While on a patrol an OPFAC comes upon a vessel which has run aground. After assessing the risk to both vessels may the facility be used to tow the stranded vessel back into safe water?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Can I include swimming as part of a public education class?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: Can I include swimming as part of a public education class?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: How Do I Dispose of My Old ODUs?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: How Do I Dispose of My Old ODUs?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: When preparing to terminate a patrol, does the coxswain notify the OPCON that they are terminating the patrol or does (s)he request permission to terminate the patrol?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: When preparing to terminate a patrol, does the coxswain notify the OPCON that they are terminating the patrol or does (s)he request permission to terminate the patrol?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: I'm not a pilot. Can I join the Auxiliary air program?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: I'm not a pilot. Can I join the Auxiliary air program?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What's the policy for wearing a life jacket on the dock when doing a patrol?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What's the policy for wearing a life jacket on the dock when doing a patrol?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: I'm a pilot. Can I join the auxiliary air program?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: I'm a pilot. Can I join the auxiliary air program?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: What mission code do I use to record hours for Meeting time, Pre-meeting prep, Travel to and from meeting as a non-officer?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: What mission code do I use to record hours for Meeting time, Pre-meeting prep, Travel to and from meeting as a non-officer?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: List two (2) reasons why someone would be ineligible to join the Auxiliary.\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: List two (2) reasons why someone would be ineligible to join the Auxiliary.\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: How many crew are required for a patrol vessel of 30 ft?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: How many crew are required for a patrol vessel of 30 ft?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: What mission code do I use to record hours for studying?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: What mission code do I use to record hours for studying?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Name one of the elected leader positions for a Flotilla.\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: Name one of the elected leader positions for a Flotilla.\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: What flags must an Auxiliary vessel display when on patrol?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: What flags must an Auxiliary vessel display when on patrol?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Do any programs have physical requirements, and if so, what are they?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: Do any programs have physical requirements, and if so, what are they?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: What is the minimum number of inspections necessary for an Auxiliarist to become an Assistant Container Inspector?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What is the minimum number of inspections necessary for an Auxiliarist to become an Assistant Container Inspector?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Is AUXPAD considered an operational mission?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Is AUXPAD considered an operational mission?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: Who can do my uniform inspection?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Who can do my uniform inspection?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: What do you do if you are in your car and colors are sounded?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: What do you do if you are in your car and colors are sounded?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: When do we, as Auxiliarists, salute other persons in the armed forces?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: When do we, as Auxiliarists, salute other persons in the armed forces?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Describe the organizational structure of a flotilla, division and district.\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Describe the organizational structure of a flotilla, division and district.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Which hand is used to render a proper salute?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Which hand is used to render a proper salute?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: What are the four administrative/supervisory levels of the Auxiliary organization.\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What are the four administrative/supervisory levels of the Auxiliary organization.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Describe the procedures to properly salute a senior officer.\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Describe the procedures to properly salute a senior officer.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Name one of the appointed leader positions.\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Name one of the appointed leader positions.\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Accuracy eval: retrieving result for question: What is an appointed leader?\n",
      "Passing result to LLM to grade accuracy\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: What is an appointed leader?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: I'm qualified as an AV, is there a ribbon for the qualification?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: I'm qualified as an AV, is there a ribbon for the qualification?\n",
      "Passing result to LLM to grade truth\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: What are the requirements to join the Auxiliary?\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: What are the requirements to join the Auxiliary?\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: When walking abreast and overtaking a senior officer what should you do?\n",
      "Retrieved context: 5 documents.\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: When walking abreast and overtaking a senior officer what should you do?\n",
      "Passing result to LLM to grade truth\n",
      "Retrieved context: 5 documents.\n",
      "Accuracy eval: retrieving result for question: Describe the purpose of the flotilla and review some of the core activities that support the business of the Flotilla.\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: Describe the purpose of the flotilla and review some of the core activities that support the business of the Flotilla.\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: Name one of the elected leader positions for the District.\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Retrieved context: 5 documents.\n",
      "Truthfulness eval: retrieving result for question: Name one of the elected leader positions for the District.\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: What is the Auxiliary Chain of Leadership and Management.\n",
      "Passing result to LLM to grade accuracy\n",
      "LLM response received\n",
      "Truthfulness eval: retrieving result for question: What is the Auxiliary Chain of Leadership and Management.\n",
      "Passing result to LLM to grade truth\n",
      "Accuracy eval: retrieving result for question: How is harassment generally defined, and who is responsible for making sure Auxiliarists do not encounter discrimination or harassment?\n",
      "Passing result to LLM to grade accuracy\n",
      "Truthfulness eval: retrieving result for question: How is harassment generally defined, and who is responsible for making sure Auxiliarists do not encounter discrimination or harassment?\n",
      "Passing result to LLM to grade truth\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    evaluate(\n",
    "        # maps the shape input from our example, which is a single-field dictionary, to the rag function we are testing, which accepts a string\n",
    "        lambda input: rag.rag(input[\"question\"]),\n",
    "        data=data,\n",
    "        client=client,  # Needed or it will use default API key\n",
    "        # accuracy_evaluator, recall_evaluator, hallucination_evaluator\n",
    "        evaluators=[accuracy_evaluator, hallucination_evaluator],\n",
    "        experiment_prefix=experiment_prefix,\n",
    "        description=experiment_description,\n",
    "        max_concurrency=1,  # Limit concurrency to avoid crashing\n",
    "        upload_results=True,  # Set to false for local testing\n",
    "        num_repetitions=1,\n",
    "        metadata=CONFIG,\n",
    "        # type: ignore    # This supresses an error\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed with error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
