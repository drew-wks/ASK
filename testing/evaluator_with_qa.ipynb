{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate RAG Quality\n",
    "##### Evaluates the app by running an experiment in Langsmith with the following metrics:\n",
    "-  Accuracy- Is the answer correct according to the ground truth answer\n",
    "-  Recall- How many of the relevant documents were retrieved\n",
    "-  Truthfulness - Did the response stray from the documents or hallucinate?\n",
    "\n",
    "Do not add code to this to run a regular rag inferences or it may put the wrong tracing project name. Use inference_tester.ipynb instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "import streamlit as st\n",
    "\n",
    "load_dotenv('/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/.env')\n",
    "\n",
    "# Add the parent directory to sys.path so you can import your modules from a subdirectory\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import rag\n",
    "from rag import CONFIG\n",
    "from langsmith.evaluation import evaluate\n",
    "from langsmith import Client\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config LangSmith if you also want the traces\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = st.secrets[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain_evaluator.ipynb on ASK main/local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "eval_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def validate_and_fix_json(raw_output: str, required_fields: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Validates and fixes JSON output, ensuring required fields are present.\n",
    "\n",
    "    Args:\n",
    "        raw_output (str): The raw JSON string from the LLM.\n",
    "        required_fields (dict): A dictionary of required fields with their default values.\n",
    "\n",
    "    Returns:\n",
    "        dict: Validated and fixed JSON output aligned with required fields.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_response = json.loads(raw_output)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSONDecodeError: {e}\")\n",
    "        print(f\"Raw output: {raw_output}\")\n",
    "\n",
    "        # Attempt common fixes\n",
    "        if raw_output.endswith('\"'):\n",
    "            raw_output += \"}\"  # Close the JSON object if improperly terminated\n",
    "        elif not raw_output.endswith('}'):\n",
    "            raw_output += '\"}'  # Add missing closing braces\n",
    "\n",
    "        # Retry parsing\n",
    "        try:\n",
    "            parsed_response = json.loads(raw_output)\n",
    "        except json.JSONDecodeError as final_e:\n",
    "            print(f\"Failed to fix JSON: {final_e}\")\n",
    "            parsed_response = {}\n",
    "\n",
    "    # Ensure required fields are present with default values\n",
    "    validated_response = {key: parsed_response.get(\n",
    "        key, default) for key, default in required_fields.items()}\n",
    "\n",
    "    return validated_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt_accuracy = prompt = hub.pull(\n",
    "    \"cot_qa_drew\")\n",
    "\n",
    "\n",
    "def accuracy_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for detecting generation accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Inputs to Evaluator from Eval set\n",
    "    query = example.inputs[\"question\"]\n",
    "    ground_truth_answer = example.outputs[\"ground_truth_answer\"]\n",
    "\n",
    "    # Inputs to Evaluator from RAG output\n",
    "    prediction = run.outputs.get(\"answer\")\n",
    "    if prediction is None:\n",
    "        print(f\"'answer' key is missing in outputs: {run.outputs}\")\n",
    "\n",
    "    llm = ChatOpenAI(model=eval_model, temperature=0,\n",
    "                     tags=[\"accuracy_evaluator\"])\n",
    "\n",
    "    # Define the grader\n",
    "    answer_grader = grade_prompt_accuracy | llm\n",
    "\n",
    "    # Get score by passing populated prompt to the evaluator\n",
    "    # The prompt template takes in \"query\", \"ground_truth_answer\", \"answer\" as inputs\n",
    "    grader_response = answer_grader.invoke({\"query\": query,\n",
    "                                           \"ground_truth_answer\": ground_truth_answer,\n",
    "                                            \"student_answer\": prediction}\n",
    "                                           )\n",
    "\n",
    "    required_fields = {\n",
    "        \"correctness\": None,  # Default correctness value\n",
    "        \"explanation\": \"No explanation provided.\"  # Default explanation\n",
    "    }\n",
    "\n",
    "    validated_response = validate_and_fix_json(\n",
    "        grader_response, required_fields)\n",
    "\n",
    "    correctness = validated_response[\"correctness\"]\n",
    "    explanation = validated_response[\"explanation\"]\n",
    "\n",
    "    return {\n",
    "        \"key\": \"Accuracy\",\n",
    "        \"score\": correctness,  # Numerical score expected by the evaluator\n",
    "        \"value\": \"Correct\" if correctness == 1 else \"Incorrect\",  # Optional categorical value\n",
    "        \"comment\": explanation,  # Additional metadata\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt_recall = prompt = hub.pull(\n",
    "    \"recall_drew\")\n",
    "\n",
    "\n",
    "def recall_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for checing the retrieved documents against the question\n",
    "    \"\"\"\n",
    "\n",
    "    # Inputs to Evaluator from Eval set\n",
    "    query = example.inputs[\"question\"]\n",
    "\n",
    "    # Inputs to Evaluator from RAG output\n",
    "    documents = run.outputs.get(\"context\")\n",
    "    if documents is None:\n",
    "        print(f\"'context' key is missing in outputs: {run.outputs}\")\n",
    "    sources = run.outputs.get(\"sources\")\n",
    "    if sources is None:\n",
    "        print(f\"'sources' key is missing in outputs: {run.outputs}\")\n",
    "\n",
    "    # LLM grader\n",
    "    # other models gpt-4-turbo gpt-4o-mini\n",
    "    llm = ChatOpenAI(model=eval_model, temperature=0,\n",
    "                     tags=[\"recall_evaluator\"])\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_recall | llm\n",
    "\n",
    "    # Get score by passing populated prompt to the evaluator\n",
    "    # The evaluator template expects \"documents\" as input\n",
    "    # The evaluator returns \"Score\" (int) and \"Explanation\" (str) as output\n",
    "    grader_response = answer_grader.invoke({\"query\": query,\n",
    "                                            \"documents\": documents})\n",
    "\n",
    "    required_fields = {\n",
    "        \"Score\": 0,  # Default value\n",
    "        \"Explanation\": \"No explanation provided.\"  # Default value\n",
    "    }\n",
    "\n",
    "    validated_response = validate_and_fix_json(\n",
    "        grader_response, required_fields)\n",
    "    score = validated_response[\"Score\"]\n",
    "    explanation = validated_response[\"Explanation\"]\n",
    "\n",
    "    return {\"key\": \"Recall\", \"score\": score, \"sources\": sources, \"comment\": explanation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt_truthfulness = prompt = hub.pull(\n",
    "    \"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "\n",
    "def hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for detecting generation hallucinations\n",
    "    \"\"\"\n",
    "\n",
    "    # Inputs to Evaluator from Eval set\n",
    "    input_question = example.inputs[\"question\"]\n",
    "\n",
    "    # Inputs to Evaluator from RAG output\n",
    "    documents = run.outputs.get(\"context\")\n",
    "    if not documents:\n",
    "        print(\n",
    "            f\"No documents retrieved. Skipping grading. Outputs: {run.outputs}\")\n",
    "        return {\n",
    "            \"key\": \"Truthfulness\",\n",
    "            \"score\": 0,  # Or any default score you'd prefer for empty context\n",
    "            \"comment\": \"No relevant documents were found to evaluate the answer.\"\n",
    "        }\n",
    "\n",
    "    prediction = run.outputs.get(\"answer\")\n",
    "    if not prediction:\n",
    "        print(f\"'answer' key is missing in outputs: {run.outputs}\")\n",
    "        return {\n",
    "            \"key\": \"Truthfulness\",\n",
    "            \"score\": 0,\n",
    "            \"comment\": \"No answer provided to evaluate.\"\n",
    "        }\n",
    "\n",
    "    # LLM grader\n",
    "    # other models gpt-4-turbo gpt-4o-mini\n",
    "    llm = ChatOpenAI(model=eval_model, temperature=0,\n",
    "                     tags=[\"hallucination_evaluator\"])\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_truthfulness | llm\n",
    "\n",
    "    # Get score by passing populated prompt to the evaluator\n",
    "    # The evaluator template expects \"documents\" and \"student_answer\" as inputs\n",
    "    # The evaluator returns \"Score\" (int) and \"Explanation\" (str) as output\n",
    "    grader_response = answer_grader.invoke({\"documents\": documents,\n",
    "                                            \"student_answer\": prediction})\n",
    "\n",
    "    required_fields = {\n",
    "        \"Score\": None,  # Default value\n",
    "        \"Explanation\": \"No explanation provided.\"  # Default value\n",
    "    }\n",
    "\n",
    "    validated_response = validate_and_fix_json(\n",
    "        grader_response, required_fields)\n",
    "\n",
    "    score = validated_response[\"Score\"]\n",
    "    explanation = validated_response[\"Explanation\"]\n",
    "\n",
    "    return {\"key\": \"Truthfulness\", \"score\": score, \"comment\": explanation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config your Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"ASK-groundtruth-v2\"\n",
    "# ASK-groundtruth_v1   initial_EDA\n",
    "\n",
    "data = dataset_name\n",
    "\n",
    "# I don't think I need this one anymore\n",
    "# data = client.list_examples(dataset_name=dataset_name, splits=[\"1_question\"])\n",
    "data = client.list_examples(dataset_name=dataset_name, example_ids=[\n",
    "                            \"2eea461c-3653-4c36-961f-256c70ee6268\"])\n",
    "\n",
    "# experiment_prefix = \"ASK_Eval_code_whichpromptisbroke\"\n",
    "experiment_prefix = \"ASK_CLART_ContextualCompressionRetriever-gpt-4o-mini\"\n",
    "\n",
    "experiment_description = \"Testing ContextualCompressionRetriever. \\n\\nNAMING CONVENTION\\nAppName_TestMetrics_TestVariables \\nExample: ASK_ART_llm-gpt-4o-mini\\nTest metrics are CLART = Cost, Latency, Accuracy, Recall, Truthfulness. Test Variable is gpt-4o-mini which we will compare against some other llm. Other example of TestMetrics could be Eval_cost, App_cost, App_time, etc.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Evaluation\n",
    " OpenAI API pricing is [here.](https://openai.com/api/pricing/)  \n",
    " Your billing is [here.](https://platform.openai.com/settings/organization/usage/activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'ASK_CLART_ContextualCompressionRetriever-gpt-4o-mini-8e4b21f1' at:\n",
      "https://smith.langchain.com/o/3941ecea-6957-508c-9f4f-08ed62dc7d61/datasets/0b24ff94-f4f0-4197-89f3-765f835936c9/compare?selectedSessions=d6955dcd-2ab8-425f-ac27-777358b765f3\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9102e1a2e44d4f378329856c1464af3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 19:59:25.644 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-17 19:59:25.669 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/drew_wilkins/Drews_Files/Drew/Python/Repositories/ASK/.venv-main/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-01-17 19:59:25.669 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-17 19:59:25.670 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-17 19:59:26.175 Thread 'Thread-10': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-17 19:59:26.176 Thread 'Thread-10': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-17 19:59:26.728 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-17 19:59:26.729 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context: 0 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator accuracy_evaluator> on run 6e99bc68-6ad0-4531-8cf2-b4cac7de3616: TypeError('the JSON object must be str, bytes or bytearray, not dict')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/drew_wilkins/Drews_Files/Drew/Python/Repositories/ASK/.venv-main/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/drew_wilkins/Drews_Files/Drew/Python/Repositories/ASK/.venv-main/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/Users/drew_wilkins/Drews_Files/Drew/Python/Repositories/ASK/.venv-main/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/drew_wilkins/Drews_Files/Drew/Python/Repositories/ASK/.venv-main/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/bd/g1lf841s4vv62bsng6h9bbkh0000gn/T/ipykernel_38093/2515283498.py\", line 37, in accuracy_evaluator\n",
      "    validated_response = validate_and_fix_json(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/bd/g1lf841s4vv62bsng6h9bbkh0000gn/T/ipykernel_38093/816149830.py\", line 17, in validate_and_fix_json\n",
      "    parsed_response = json.loads(raw_output)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py\", line 339, in loads\n",
      "    raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n",
      "TypeError: the JSON object must be str, bytes or bytearray, not dict\n",
      "Error running evaluator <DynamicRunEvaluator recall_evaluator> on run 6e99bc68-6ad0-4531-8cf2-b4cac7de3616: TypeError('the JSON object must be str, bytes or bytearray, not dict')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/drew_wilkins/Drews_Files/Drew/Python/Repositories/ASK/.venv-main/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/drew_wilkins/Drews_Files/Drew/Python/Repositories/ASK/.venv-main/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/Users/drew_wilkins/Drews_Files/Drew/Python/Repositories/ASK/.venv-main/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/drew_wilkins/Drews_Files/Drew/Python/Repositories/ASK/.venv-main/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/bd/g1lf841s4vv62bsng6h9bbkh0000gn/T/ipykernel_38093/1946198281.py\", line 40, in recall_evaluator\n",
      "    validated_response = validate_and_fix_json(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/bd/g1lf841s4vv62bsng6h9bbkh0000gn/T/ipykernel_38093/816149830.py\", line 17, in validate_and_fix_json\n",
      "    parsed_response = json.loads(raw_output)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py\", line 339, in loads\n",
      "    raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n",
      "TypeError: the JSON object must be str, bytes or bytearray, not dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No documents retrieved. Skipping grading. Outputs: {'answer': \"To dispose of your old Operational Dress Uniforms (ODUs), you can follow these steps:\\n\\n1. **Check for Local Regulations**: Before disposal, check with your local waste management authority or recycling center for any specific guidelines on disposing of uniforms or textiles. Some areas have regulations regarding textile waste.\\n\\n2. **Donation**: If the ODUs are still in good condition, consider donating them to organizations that accept military uniforms. This can include local veterans' organizations, thrift stores, or charities that support military personnel.\\n\\n3. **Recycling**: If the uniforms are too worn for reuse, look for textile recycling programs in your area. Many municipalities offer textile recycling services that can help reduce waste.\\n\\n4. **Disposal**: If no other options are available, the uniforms can be disposed of in the regular trash. However, it's advisable to ensure they are cut up or altered in a way that makes them unrecognizable as military uniforms to avoid misuse.\\n\\n5. **Regulations**: Ensure that you comply with any specific disposal regulations related to military uniforms, as there may be guidelines on how to properly dispose of items that represent military service.\\n\\nBy following these steps, you can responsibly dispose of your old ODUs while adhering to local regulations and considerations.\", 'sources': [], 'user_question': 'How Do I Dispose of My Old ODUs?', 'enriched_question': 'How Do I Dispose of My Old ODUs?', 'context': [], 'llm_sources': ['Local Waste Management Guidelines', 'Military Uniform Disposal Regulations', 'Textile Recycling Programs']}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.answer</th>\n",
       "      <th>outputs.sources</th>\n",
       "      <th>outputs.user_question</th>\n",
       "      <th>outputs.enriched_question</th>\n",
       "      <th>outputs.context</th>\n",
       "      <th>outputs.llm_sources</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.ground_truth_answer</th>\n",
       "      <th>reference.ground_truth_sources</th>\n",
       "      <th>feedback.Accuracy</th>\n",
       "      <th>feedback.Recall</th>\n",
       "      <th>feedback.Truthfulness</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How Do I Dispose of My Old ODUs?</td>\n",
       "      <td>To dispose of your old Operational Dress Unifo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>How Do I Dispose of My Old ODUs?</td>\n",
       "      <td>How Do I Dispose of My Old ODUs?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Local Waste Management Guidelines, Military U...</td>\n",
       "      <td>None</td>\n",
       "      <td>Uniform items that are no longer serviceable, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>9.444729</td>\n",
       "      <td>2eea461c-3653-4c36-961f-256c70ee6268</td>\n",
       "      <td>6e99bc68-6ad0-4531-8cf2-b4cac7de3616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults ASK_CLART_ContextualCompressionRetriever-gpt-4o-mini-8e4b21f1>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    # maps the shape input from our example, which is a single-field dictionary, to the rag function we are testing, which accepts a string\n",
    "    lambda input: rag.rag(input[\"question\"]),\n",
    "    data=data,\n",
    "    # accuracy_evaluator, recall_evaluator, hallucination_evaluator\n",
    "    evaluators=[accuracy_evaluator, recall_evaluator, hallucination_evaluator],\n",
    "    experiment_prefix=experiment_prefix,\n",
    "    description=experiment_description,\n",
    "    num_repetitions=1,\n",
    "    metadata=CONFIG,\n",
    ")  # type: ignore    # This supresses an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
