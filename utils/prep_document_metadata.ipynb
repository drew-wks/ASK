{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  This is the process for grabbing the pdfs and extracting metadat to a library catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It includes adding the metadata we are going to use. The custom metadata list is defined in this code block at the bottom. Custom fields can be defined in the custom_fields list. The functions take the list as an argument and  checks for the additional custom metadata fields and includes them in the xlsx if they are present in the PDF files.\n",
    "##### THis identifies potential duplicate PDF files, you can compute a hash (e.g., SHA-256) for each file and compare these values. Files with the same hash value are very likely to be duplicates.  \n",
    "##### With the check_pdf_issues function in place, before attempting to get metadata from a PDF, the script will first check if the PDF has issues like being encrypted or corrupt. If it's encrypted, it will attempt to decrypt using the provided password (in this case, an empty string). If it's corrupt or if there's any other issue, it will log the problem and skip the file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install ipython\n",
    "#%pip install pypdf\n",
    "\n",
    "%pip install bs4\n",
    "%pip install requests\n",
    "#%pip install openpyxl\n",
    "%pip install tabulate\n",
    "\n",
    "\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "import os\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime, date, timedelta\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/ASK/data/PDF_archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create metadata dictionary\n",
    "# Check if the path exists\n",
    "def check_dir_exists(source_directory):\n",
    "    if not os.path.exists(source_directory):\n",
    "        print(f\"Error: The path '{source_directory}' does not exist!\")\n",
    "        raise ValueError(f\"The path '{source_directory}' does not exist!\")\n",
    "\n",
    "\n",
    "def compute_pdf_hash(pdf_path):\n",
    "    '''generate a unique hash for the PDF file'''\n",
    "    doc_id = hashlib.md5()\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        for block in iter(lambda: f.read(4096), b\"\"):\n",
    "            doc_id.update(block)\n",
    "    return doc_id.hexdigest()\n",
    "\n",
    "\n",
    "def check_pdf_for_issues(pdf_path):\n",
    "    try:\n",
    "        pdf = PdfReader(pdf_path)\n",
    "        if pdf.is_encrypted:\n",
    "            print(f\"Encryption detected for {pdf_path}\")\n",
    "            pdf.decrypt(\"\")\n",
    "            print(f\"All pages accessed: {len(pdf.pages)}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Issue with {pdf_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_pdf_metadata(pdf_path):\n",
    "    '''extract all metadata fields present in the PDF file \n",
    "    along with page count and the hash into a dictionary\n",
    "    '''\n",
    "    pdf_metadata = {}\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PdfReader(f)\n",
    "        existing_pdf_metadata = reader.metadata\n",
    "        # existing pdf metadata code deleted from here\n",
    "\n",
    "        # Create the pdf metadata\n",
    "        file_name = os.path.basename(pdf_path)\n",
    "        while '.' in file_name:\n",
    "            # loop to remove mult extensions\n",
    "            file_name = os.path.splitext(file_name)[0]\n",
    "        pdf_metadata['title'] = existing_pdf_metadata.get('/Title')\n",
    "        if not pdf_metadata['title']:\n",
    "            pdf_metadata['title'] = file_name.replace('_', ' ')\n",
    "        pdf_metadata['leadership_scope'] = \"1_National\"\n",
    "        pdf_metadata['page_count'] = len(reader.pages)  # Add page count\n",
    "        creation_date_str = existing_pdf_metadata.get(\n",
    "            '/CreationDate', '')[2:10]\n",
    "        if creation_date_str:\n",
    "            created_date = datetime.strptime(creation_date_str, '%Y-%m-%dT%H:%MZ')\n",
    "        else:\n",
    "            created_date = date.today()\n",
    "        pdf_metadata['creation_date'] = created_date.strftime('%Y-%m-%dT%H:%MZ')\n",
    "        pdf_metadata['effective_date'] = created_date.strftime('%Y-%m-%dT%H:%MZ')\n",
    "        pdf_metadata['upsert_date'] = date.today().strftime('%Y-%m-%dT%H:%MZ')\n",
    "        expiration_date = created_date + timedelta(days=365.25*10)\n",
    "        pdf_metadata['expiration_date'] = expiration_date.strftime('%Y-%m-%dT%H:%MZ')\n",
    "        pdf_metadata['aux_specific'] = True\n",
    "        pdf_metadata['public_release'] = True\n",
    "        pdf_metadata['publication_number'] = pdf_metadata['Title']\n",
    "        pdf_metadata['source'] = None\n",
    "        # not curently used. Can be CG Org or Unit Number\n",
    "        pdf_metadata['organization'] = None\n",
    "        pdf_metadata['curator'] = \"Drew_Wilkins\"\n",
    "        pdf_metadata['document_id'] = compute_pdf_hash(\n",
    "            pdf_path)  # Compute and add the hash of the PDF\n",
    "        pdf_metadata['file_name'] = file_name  # add the filename\n",
    "        # this metadata is needed to write the metadata back to the pdfs\n",
    "        pdf_metadata['pdf_path'] = pdf_path\n",
    "    return pdf_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '20-10-29' does not match format '%Y%m%d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m                 all_pdfs_metadata[file] \u001b[38;5;241m=\u001b[39m pdf_metadata_dict\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_pdfs_metadata\n\u001b[0;32m---> 21\u001b[0m all_pdfs_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mmake_metadata_dict_from_pdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mmake_metadata_dict_from_pdfs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             compute_pdf_hash(pdf_path)\n\u001b[1;32m     14\u001b[0m             check_pdf_for_issues(pdf_path)\n\u001b[0;32m---> 15\u001b[0m             pdf_metadata_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_pdf_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m             all_pdfs_metadata[file] \u001b[38;5;241m=\u001b[39m pdf_metadata_dict\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_pdfs_metadata\n",
      "Cell \u001b[0;32mIn[6], line 54\u001b[0m, in \u001b[0;36mget_pdf_metadata\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m     51\u001b[0m creation_date_str \u001b[38;5;241m=\u001b[39m existing_pdf_metadata\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/CreationDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m10\u001b[39m]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m creation_date_str:\n\u001b[0;32m---> 54\u001b[0m     created_date \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreation_date_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     created_date \u001b[38;5;241m=\u001b[39m date\u001b[38;5;241m.\u001b[39mtoday()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_strptime.py:568\u001b[0m, in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_strptime_datetime\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_string, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%a\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a class cls instance based on the input string and the\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03m    format string.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m     tt, fraction, gmtoff_fraction \u001b[38;5;241m=\u001b[39m \u001b[43m_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m     tzname, gmtoff \u001b[38;5;241m=\u001b[39m tt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    570\u001b[0m     args \u001b[38;5;241m=\u001b[39m tt[:\u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m+\u001b[39m (fraction,)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_strptime.py:349\u001b[0m, in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    347\u001b[0m found \u001b[38;5;241m=\u001b[39m format_regex\u001b[38;5;241m.\u001b[39mmatch(data_string)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m found:\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime data \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not match format \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    350\u001b[0m                      (data_string, \u001b[38;5;28mformat\u001b[39m))\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_string) \u001b[38;5;241m!=\u001b[39m found\u001b[38;5;241m.\u001b[39mend():\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munconverted data remains: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    353\u001b[0m                       data_string[found\u001b[38;5;241m.\u001b[39mend():])\n",
      "\u001b[0;31mValueError\u001b[0m: time data '20-10-29' does not match format '%Y%m%d'"
     ]
    }
   ],
   "source": [
    "def make_metadata_dict_from_pdfs():\n",
    "    '''pulls the metadata from all the pdfs into a dataframe with standard formatting\n",
    "        pdfs in rows and metadata atributes in columns\n",
    "    '''\n",
    "    check_dir_exists(source_directory)\n",
    "\n",
    "    all_pdfs_metadata = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(source_directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".pdf\"):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                compute_pdf_hash(pdf_path)\n",
    "                check_pdf_for_issues(pdf_path)\n",
    "                pdf_metadata_dict = get_pdf_metadata(pdf_path)\n",
    "                all_pdfs_metadata[file] = pdf_metadata_dict\n",
    "\n",
    "    return all_pdfs_metadata\n",
    "\n",
    "\n",
    "all_pdfs_metadata = make_metadata_dict_from_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\\nBased on this dictionary...\\n {all_pdfs_metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulls the metadata from all the pdfs into a dataframe with standard formatting\n",
    "#pdfs in rows and metadata atributes in columns\n",
    "metadata_preview = pd.DataFrame(all_pdfs_metadata).transpose()\n",
    "\n",
    "print(f\"\"\"Dataframe loaded with metadata for rows, columns: {metadata_preview.shape} \\nInspect first row below....\\n\\n\"\"\")\n",
    "print(f\"\")\n",
    "# transpose to pdfs in rows and metadata in columns\n",
    "# pdfs_df_edit_me = pdfs_df.transpose()\n",
    "\n",
    "print(f\"\"\"INDEX FOR THIS ROW:             {metadata_preview.index[0]}\\n\"\"\")\n",
    "print(metadata_preview.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def make_xlsx(all_pdfs_metadata):\n",
    "    \"\"\"write dataframe to an Excel file to edit by hand.\"\"\"\n",
    "    \n",
    "    now_utc = datetime.utcnow()\n",
    "    timestamp = now_utc.strftime('%Y-%m-%dT%H:%MZ')\n",
    "    file_path = f'../docs/library_catalog/library_doc_catalog_{timestamp}.xlsx'\n",
    "    \n",
    "    # Save DataFrame to Excel. Index=True metadata_keys as row 1\n",
    "    # if Index =True then be sure to pd.read_excel( , index_col=0) when you bring it back in\n",
    "    all_pdfs_metadata.to_excel(file_path, index=True)\n",
    "\n",
    "make_xlsx(all_pdfs_metadata)\n",
    "\n",
    "print(f\"\"\" editable excel file has been posted as {file_path}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
